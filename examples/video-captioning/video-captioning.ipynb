{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Captioning With VLMs\n",
    "\n",
    "Video captioning and Q/A is far from a solved problem. Ideally, we'd have a video model that can natively process the audio and every frame and output a response. Unfortunately this is not the reality at this moment--the image frames alone would far exceed the context window of any LLM, even for a 30 second video.\n",
    "\n",
    "An alternative is to just take frames at certain intervals, and that's what we are going to do in this sample. This isn't only possible, but also an order of magnitude cheaper.\n",
    "\n",
    "The most cost-effective way to caption a video is to sample some frames at a set interval, and if available to get the transcript, or a summary of the transcript, and pass that as well. In this case we don't have the transcript, so we'll make do with some image frames that we get from a YouTube Video of Mr.Beast taking his girlfriend on a date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's install some packages. Since models hosted on Inference are compatible with the OpenAI SDK, we'll be using that to interact with the Inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt-dlp in /Users/michaelryaboy/recent-projects/inference-webhook/venv/lib/python3.9/site-packages (2025.6.9)\n",
      "Requirement already satisfied: opencv-python in /Users/michaelryaboy/recent-projects/inference-webhook/venv/lib/python3.9/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: openai in /Users/michaelryaboy/recent-projects/inference-webhook/venv/lib/python3.9/site-packages (1.84.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/michaelryaboy/recent-projects/inference-webhook/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp opencv-python openai glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the yt-dlp package to scrape 5 frames from a Mr.Beast video titled '$1 vs $500,000 Romantic Date.'\n",
    "\n",
    "Fun fact: OpenAI is one of the most active maintainers of yt-dlp because they are using it to scrape YT at scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[download]   9.1% of    1.36GiB at   16.87MiB/s ETA 01:14[out#0/image2 @ 0x13a814a30] video:2121KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: unknown\n",
      "frame=    5 fps=0.5 q=1.6 Lsize=N/A time=00:01:40.00 bitrate=N/A speed=10.1x    \n",
      "[download]   9.1% of    1.36GiB at   15.44MiB/s ETA 01:21\n",
      "\n",
      "ERROR: unable to write data: [Errno 32] Broken pipe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p keyframes\n",
    "yt-dlp -f bestvideo[ext=mp4] -o - \"https://www.youtube.com/watch?v=hTSaweR8qMI\" \\\n",
    "  | ffmpeg -i pipe: \\\n",
    "           -vf fps=.05 \\\n",
    "           -frames:v 5 \\\n",
    "           keyframes/keyframe_%02d.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some packages. Not much to see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from glob import glob           # <–– grabs the function glob()\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Video Captioning with Inference API\n",
    "\n",
    "### Getting Your API Key\n",
    "\n",
    "First thing - grab your Inference API key from [https://inference.net/dashboard/api-keys](https://inference.net/dashboard/api-keys).\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Set your base URL to `https://api.inference.net/v1` so requests route to Inference instead of OpenAI. We're using the `google/gemma-3-27b-instruct/bf-16` model - it's compact but handles images really well.\n",
    "\n",
    "### System Message\n",
    "\n",
    "Configure your system message to tell the model it's a captioning service. Basically you're saying \"your job is to analyze video frames and write captions describing what's happening.\"\n",
    "\n",
    "### Image Quality Matters\n",
    "\n",
    "This is important - use the highest quality frames you can. The model does well with text recognition and fine details, but only if it can actually see them clearly. Blurry or low-res images will hurt your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"INFERENCE_API_KEY\")\n",
    "MODEL   = \"google/gemma-3-27b-instruct/bf-16\"\n",
    "SYSTEM_MSG = \"\"\"\n",
    "You are a JSON-only image analysis API specializing in YouTube keyframes.\n",
    "Generate one concise caption that describes what's happening across all these frames.\n",
    "Respond only with a JSON object:\n",
    "\n",
    "{\"caption\": \"…\"}\n",
    "\"\"\".strip()\n",
    "\n",
    "client = OpenAI(base_url=\"https://api.inference.net/v1\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass images to our VLM API, we need to first encode them into base64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uris = []\n",
    "for filepath in sorted(glob(\"keyframes/*.jpg\")):   # now glob(...) works!\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    data_uris.append(f\"data:image/jpeg;base64,{b64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate an image. We'll define a json schema so the model has to give us a valid caption.\n",
    "\n",
    "To learn more about json schemas check out the [structured output docs](https://docs.inference.net/features/structured-outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{\\\"caption\\\": \\\"A couple experiences a date night at an amusement park with escalating costs, ultimately leading to a close moment between them.\\\"}\"\n"
     ]
    }
   ],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Here are 5 keyframes from a YouTube video. Generate a single caption.\"},\n",
    "                *[\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": uri}}\n",
    "                    for uri in data_uris\n",
    "                ]\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    response_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"video_caption\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"caption\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A concise caption describing what's happening across all the video frames\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"caption\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "    }\n",
    "}\n",
    ")\n",
    "\n",
    "# — OUTPUT RESULT —\n",
    "print(json.dumps(resp.choices[0].message.content, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got a good caption. Not as good as the one we would get with the full transcript, but still impressive, considering we passed only 5 images.\n",
    "\n",
    "In production, we may want to fine-tune a model to do this task even more cheaply and effectively, depending on the scale!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
