{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Image Captioning at Scale with Vision LLMs.\n",
    "\n",
    "Vision models excel at understanding and describing images. They can grok the contents of images similar to how a human can, and can find patterns, objects, and even process many images at a time.\n",
    "\n",
    "In this notebook we'll learn how to caption images, and also extract insights from a large number of images.\n",
    "\n",
    "We'll be using a VLMs (Vision Language Model) to create a dataset of ugly and beautiful websites. For simplicity our dataset will only consist of a couple hundred images, but an AI lab seeking to improve LLM design ability may scale it to millions or billions of web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setting Up Your Captioning Pipeline\n",
    "\n",
    "First we download some libraries. We interact with Inference's API through OpenAI's official API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/michaelryaboy/recent-projects/inference-webhook/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai requests pillow datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can connect to Inference using the OpenAI SDK.\n",
    "\n",
    "We pass the Inference.net API baseurl, and make sure we have our API key set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from io import BytesIO\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://batch.inference.net/v1\",\n",
    "    api_key=os.getenv(\"INFERENCE_API_KEY\"),\n",
    ")\n",
    "\n",
    "VISION_MODEL = \"google/gemma-3-27b-instruct/bf-16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load the training split of the website screenshots dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 screenshots\n"
     ]
    }
   ],
   "source": [
    "N = 200\n",
    "ds = load_dataset(\n",
    "    \"Zexanima/website_screenshots_image_dataset\",\n",
    "    split=\"train\",                                   # ⚠️ use “test” later for eval\n",
    "    streaming=False                                  # stream=True ≈ zero‑RAM, slower\n",
    ").select(range(N))\n",
    "print(\"Loaded\", len(ds), \"screenshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function to turn images into a data-URI so that we can pass them to our VLM API endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data_uri(sample):\n",
    "    if isinstance(sample, Image.Image):\n",
    "        img = sample.convert(\"RGB\")\n",
    "        buf = BytesIO()\n",
    "        img.save(buf, format=\"PNG\", optimize=True)\n",
    "        data = buf.getvalue()\n",
    "    else:                                             # remote URL\n",
    "        data = requests.get(sample).content\n",
    "    b64 = base64.b64encode(data).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{b64}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Your First Image Captioning Request\n",
    "\n",
    "Let's start by captioning a few sample images. We'll download some images and convert them to base64 data URIs so our LLM API can process them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL size ≈ 125440 kB\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
    "    You are a senior product‑designer assistant evaluating full‑page website\n",
    "    screenshots.  Perform **two independent judgements**:\n",
    "\n",
    "    ────────────────────────────────────────────────────────────────────\n",
    "    1. landing_page (boolean)\n",
    "       ▸ TRUE  – the screenshot looks like the FIRST page a visitor sees\n",
    "                 (hero section or marquee visual, primary navigation bar,\n",
    "                 clear top‑level call‑to‑action, little or no scroll offset).\n",
    "       ▸ FALSE – any interior page, modal, or state that presumes prior\n",
    "                 navigation (pricing tables, blog posts, dashboards, etc.).\n",
    "    ────────────────────────────────────────────────────────────────────\n",
    "    2. aesthetic  (integer 1‑5)\n",
    "       Rate overall visual polish **at the moment the screenshot was taken**.\n",
    "       Use the rubric below; intermediate numbers are **not permitted**.\n",
    "\n",
    "         1 ▪︎ Poor   – chaotic layout, clashing colours, unreadable text,\n",
    "                      obvious placeholder or broken assets.\n",
    "         2 ▪︎ Below Avg – dated styling, inconsistent spacing/alignment,\n",
    "                      low‑contrast elements, generic stock imagery.\n",
    "         3 ▪︎ Average  – competent but ordinary; standard template vibes,\n",
    "                      minor visual debts allowed, no major UX anti‑patterns.\n",
    "         4 ▪︎ Good    – clean hierarchy, harmonious palette & typography,\n",
    "                      responsive‑looking grid, purposeful imagery/icons.\n",
    "         5 ▪︎ Excellent – editorial‑grade art direction, meticulous spacing,\n",
    "                      delightful micro‑details, persuasive visual storytelling.\n",
    "\n",
    "       ✱ Ignore personal taste; judge by professional UI/UX heuristics\n",
    "         (legibility, balance, affordance, consistency, brand presence).\n",
    "\n",
    "    OUTPUT FORMAT  (strict)\n",
    "    ------------------------\n",
    "    {\n",
    "      \"landing_page\": <true|false>,\n",
    "      \"aesthetic\":    <integer 1‑5>\n",
    "    }\n",
    "\n",
    "    • Return **JSON ONLY** – no comments, no extra keys, no trailing commas.\n",
    "    • If unsure, choose the **more conservative** (lower) aesthetic score.\n",
    "\"\"\").strip()\n",
    "\n",
    "schema = {                                 # strict JSON schema = no post‑cleanup\n",
    "    \"name\": \"webscreen_classification\",\n",
    "    \"strict\": True,\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"landing_page\": {\"type\": \"boolean\"},\n",
    "            \"aesthetic\":    {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5}\n",
    "        },\n",
    "        \"required\": [\"landing_page\", \"aesthetic\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "lines = []\n",
    "for row in ds:\n",
    "    data_uri = to_data_uri(row[\"image\"])\n",
    "    \n",
    "    body = {\n",
    "        \"model\": VISION_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                {\"type\": \"text\",      \"text\": \"Classify this screenshot.\"}\n",
    "            ]}\n",
    "        ],\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": schema\n",
    "        },\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "    \n",
    "    lines.append(json.dumps({\n",
    "        \"custom_id\": f\"img_{row['image_id']}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": body\n",
    "    }))\n",
    "\n",
    "jsonl_blob = \"\\n\".join(lines)\n",
    "print(\"JSONL size ≈\", len(jsonl_blob)//1024, \"kB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "    file=BytesIO(jsonl_blob.encode('utf-8')),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: AxE23ijsKrh9f02krLIm4 Status: in_progress\n"
     ]
    }
   ],
   "source": [
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "print(\"Batch ID:\", batch.id, \"Status:\", batch.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch status: in_progress\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 10 seconds before polling again\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal batch object:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Poll for batch status until it's no longer \"in_progress\"\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Batch status:\", batch.status)\n",
    "    if batch.status != \"in_progress\":\n",
    "        break\n",
    "    time.sleep(10)  # Wait 10 seconds before polling again\n",
    "\n",
    "print(\"Final batch object:\")\n",
    "print(batch)\n",
    "\n",
    "output_file = client.files.content(batch.output_file_id)\n",
    "print(output_file.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
