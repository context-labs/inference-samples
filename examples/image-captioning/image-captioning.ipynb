{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Image Captioning at Scale with Vision LLMs.\n",
    "\n",
    "Vision models excel at understanding and describing images. They can grok the contents of images similar to how a human can, and can find patterns, objects, and even process many images at a time.\n",
    "\n",
    "In this notebook we'll learn how to caption images, and also extract insights from a large number of images.\n",
    "\n",
    "We'll be using a VLMs (Vision Language Model) to create a dataset of ugly and beautiful websites. For simplicity our dataset will only consist of a couple hundred images, but an AI lab seeking to improve LLM design ability may scale it to millions or billions of web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setting Up Your Captioning Pipeline\n",
    "\n",
    "First we download some libraries. We interact with Inference's API through OpenAI's official API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/michaelryaboy/recent-projects/inference-webhook/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai requests pillow datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can connect to Inference using the OpenAI SDK.\n",
    "\n",
    "We pass the Inference.net API baseurl, and make sure we have our API key set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from io import BytesIO\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://batch.inference.net/v1\",\n",
    "    api_key=os.getenv(\"INFERENCE_API_KEY\"),\n",
    ")\n",
    "\n",
    "VISION_MODEL = \"google/gemma-3-27b-instruct/bf-16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load the training split of the website screenshots dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 screenshots\n"
     ]
    }
   ],
   "source": [
    "N = 200\n",
    "ds = load_dataset(\n",
    "    \"Zexanima/website_screenshots_image_dataset\",\n",
    "    split=\"train\",                                   # ⚠️ use “test” later for eval\n",
    "    streaming=False                                  # stream=True ≈ zero‑RAM, slower\n",
    ").select(range(N))\n",
    "print(\"Loaded\", len(ds), \"screenshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function to turn images into a data-URI so that we can pass them to our VLM API endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data_uri(sample):\n",
    "    if isinstance(sample, Image.Image):\n",
    "        img = sample.convert(\"RGB\")\n",
    "        buf = BytesIO()\n",
    "        img.save(buf, format=\"PNG\", optimize=True)\n",
    "        data = buf.getvalue()\n",
    "    else:                                             # remote URL\n",
    "        data = requests.get(sample).content\n",
    "    b64 = base64.b64encode(data).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{b64}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Your First Image Captioning Request\n",
    "\n",
    "Let's start by captioning a few sample images. We'll download some images and convert them to base64 data URIs so our LLM API can process them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL size ≈ 125440 kB\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
    "    You are a senior product‑designer assistant evaluating full‑page website\n",
    "    screenshots.  Perform **two independent judgements**:\n",
    "\n",
    "    ────────────────────────────────────────────────────────────────────\n",
    "    1. landing_page (boolean)\n",
    "       ▸ TRUE  – the screenshot looks like the FIRST page a visitor sees\n",
    "                 (hero section or marquee visual, primary navigation bar,\n",
    "                 clear top‑level call‑to‑action, little or no scroll offset).\n",
    "       ▸ FALSE – any interior page, modal, or state that presumes prior\n",
    "                 navigation (pricing tables, blog posts, dashboards, etc.).\n",
    "    ────────────────────────────────────────────────────────────────────\n",
    "    2. aesthetic  (integer 1‑5)\n",
    "       Rate overall visual polish **at the moment the screenshot was taken**.\n",
    "       Use the rubric below; intermediate numbers are **not permitted**.\n",
    "\n",
    "         1 ▪︎ Poor   – chaotic layout, clashing colours, unreadable text,\n",
    "                      obvious placeholder or broken assets.\n",
    "         2 ▪︎ Below Avg – dated styling, inconsistent spacing/alignment,\n",
    "                      low‑contrast elements, generic stock imagery.\n",
    "         3 ▪︎ Average  – competent but ordinary; standard template vibes,\n",
    "                      minor visual debts allowed, no major UX anti‑patterns.\n",
    "         4 ▪︎ Good    – clean hierarchy, harmonious palette & typography,\n",
    "                      responsive‑looking grid, purposeful imagery/icons.\n",
    "         5 ▪︎ Excellent – editorial‑grade art direction, meticulous spacing,\n",
    "                      delightful micro‑details, persuasive visual storytelling.\n",
    "\n",
    "       ✱ Ignore personal taste; judge by professional UI/UX heuristics\n",
    "         (legibility, balance, affordance, consistency, brand presence).\n",
    "\n",
    "    OUTPUT FORMAT  (strict)\n",
    "    ------------------------\n",
    "    {\n",
    "      \"landing_page\": <true|false>,\n",
    "      \"aesthetic\":    <integer 1‑5>\n",
    "    }\n",
    "\n",
    "    • Return **JSON ONLY** – no comments, no extra keys, no trailing commas.\n",
    "    • If unsure, choose the **more conservative** (lower) aesthetic score.\n",
    "\"\"\").strip()\n",
    "\n",
    "schema = {                                 # strict JSON schema = no post‑cleanup\n",
    "    \"name\": \"webscreen_classification\",\n",
    "    \"strict\": True,\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"landing_page\": {\"type\": \"boolean\"},\n",
    "            \"aesthetic\":    {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5}\n",
    "        },\n",
    "        \"required\": [\"landing_page\", \"aesthetic\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "lines = []\n",
    "for row in ds:\n",
    "    data_uri = to_data_uri(row[\"image\"])\n",
    "    \n",
    "    body = {\n",
    "        \"model\": VISION_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                {\"type\": \"text\",      \"text\": \"Classify this screenshot.\"}\n",
    "            ]}\n",
    "        ],\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": schema\n",
    "        },\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "    \n",
    "    lines.append(json.dumps({\n",
    "        \"custom_id\": f\"img_{row['image_id']}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": body\n",
    "    }))\n",
    "\n",
    "jsonl_blob = \"\\n\".join(lines)\n",
    "print(\"JSONL size ≈\", len(jsonl_blob)//1024, \"kB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<title>Error</title>\n</head>\n<body>\n<pre>Cannot POST /v1/files</pre>\n</body>\n</html>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_input_file \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonl_blob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpurpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/openai/resources/files.py:118\u001b[0m, in \u001b[0;36mFiles.create\u001b[0;34m(self, file, purpose, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[1;32m    117\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultipart/form-data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFileObject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/openai/_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/openai/_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1036\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: <!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<title>Error</title>\n</head>\n<body>\n<pre>Cannot POST /v1/files</pre>\n</body>\n</html>"
     ]
    }
   ],
   "source": [
    "batch_input_file = client.files.create(\n",
    "    file=BytesIO(jsonl_blob.encode('utf-8')),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: AxE23ijsKrh9f02krLIm4 Status: in_progress\n"
     ]
    }
   ],
   "source": [
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "print(\"Batch ID:\", batch.id, \"Status:\", batch.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Poll for batch status until it's no longer \"in_progress\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mretrieve(\u001b[43mbatch\u001b[49m\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch status:\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Poll for batch status until it's no longer \"in_progress\"\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Batch status:\", batch.status)\n",
    "    if batch.status != \"in_progress\":\n",
    "        break\n",
    "    time.sleep(10)  # Wait 10 seconds before polling again\n",
    "\n",
    "print(\"Final batch object:\")\n",
    "print(batch)\n",
    "\n",
    "output_file = client.files.content(batch.output_file_id)\n",
    "print(output_file.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
