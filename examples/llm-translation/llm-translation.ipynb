{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LLM Translation at Scale with Inference.net Batch API\n",
        "\n",
        "LLMs are remarkably good at translation. It doesn't take a particularly strong LLM to perform most translations: a small 8-30B parameter model is more than strong enough for translating between most languages. The OpenRouter leaderboard shows the most popular models used for translation are tiny and fast--which allow you to translate very large amounts of text remarkably cheaply.\n",
        "\n",
        "![image](openrouter.png)\n",
        "\n",
        "For most translation tasks, the specific cheap model/provider you use isn't particularly important. But some translation jobs scale to the hundreds of millions to tens of trillions of tokens, and at that point price and rate limits become a factor. \n",
        "\n",
        "This is where Inference excels: we serve models extremely cheaply and have no rate limits for time-insensitive batch jobs like this.\n",
        "\n",
        "Here's how you can get started with LLM translation:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setting Up Your Translation Pipeline\n",
        "\n",
        "The beauty of using Inference.net is that it's compatible with the OpenAI SDK, so you can get started in seconds. Just point the client at our batch endpoint:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai -q\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://batch.inference.net/v1\",\n",
        "    api_key=os.getenv(\"INFERENCE_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Your First Translation Batch\n",
        "\n",
        "Let's say you need to translate a batch of product descriptions. With the Batch API, you prepare all your requests in a JSONL file where each line is a complete translation request:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created batch file with 5 translation requests\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Some product descriptions to translate\n",
        "documents = [\n",
        "    \"High-quality wireless headphones with noise cancellation\",\n",
        "    \"Ergonomic office chair with lumbar support\", \n",
        "    \"Smart home thermostat with energy-saving features\",\n",
        "    \"Professional camera with 4K video recording\",\n",
        "    \"Portable power bank with fast charging\"\n",
        "]\n",
        "\n",
        "# Create the batch file\n",
        "with open(\"translation_batch.jsonl\", \"w\") as f:\n",
        "    for idx, doc in enumerate(documents):\n",
        "        request = {\n",
        "            \"custom_id\": f\"translation-{idx}\",\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"meta-llama/llama-3.2-1b-instruct/fp-8\",\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a professional translator. Translate the following English text to Spanish, preserving the tone and meaning. Only translate the text, do not add any other text.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": doc\n",
        "                    }\n",
        "                ],\n",
        "                \"max_tokens\": 100,\n",
        "                \"temperature\": 0.3  # Lower temperature for consistent translations\n",
        "            }\n",
        "        }\n",
        "        f.write(json.dumps(request) + \"\\n\")\n",
        "\n",
        "print(f\"Created batch file with {len(documents)} translation requests\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "The key here is that each request is self-contained. You specify the model, the system prompt (your translation instructions), and the text to translate. Setting temperature to 0.3 gives you consistent, professional translations without too much creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Launching the Job\n",
        "\n",
        "Once your batch file is ready, it's a two-step process: upload the file, then create the batch job.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch job started: dW09IT9URRp-tQiK_4vjr\n"
          ]
        }
      ],
      "source": [
        "# Upload the file\n",
        "with open(\"translation_batch.jsonl\", \"rb\") as f:\n",
        "    batch_file = client.files.create(\n",
        "        file=f,\n",
        "        purpose=\"batch\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Create the batch job\n",
        "batch = client.batches.create(\n",
        "    input_file_id=batch_file.id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"job_type\": \"translation\",\n",
        "        \"document_count\": str(len(documents))\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Batch job started: {batch.id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "The batch starts processing immediately. For small jobs like this, it'll complete in seconds. For massive jobs with millions of tokens, it might take a few hours--but that's still faster than hitting rate limits with synchronous APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Getting Your Translations\n",
        "\n",
        "Once the batch completes, you download the results and parse them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Output empty ‚Äì showing error log lines:\n",
            "\n",
            "{\"id\": \"dW09IT9URRp-tQiK_4vjr\", \"custom_id\": \"translation-4\", \"response\": null, \"error\": {\"code\": \"inference_failed\", \"message\": \"Maximum retries reached\"}}\n",
            "{\"id\": \"dW09IT9URRp-tQiK_4vjr\", \"custom_id\": \"translation-3\", \"response\": null, \"error\": {\"code\": \"inference_failed\", \"message\": \"Maximum retries reached\"}}\n",
            "{\"id\": \"dW09IT9URRp-tQiK_4vjr\", \"custom_id\": \"translation-2\", \"response\": null, \"error\": {\"code\": \"inference_failed\", \"message\": \"Maximum retries reached\"}}\n",
            "{\"id\": \"dW09IT9URRp-tQiK_4vjr\", \"custom_id\": \"translation-1\", \"response\": null, \"error\": {\"code\": \"inference_failed\", \"message\": \"Maximum retries reached\"}}\n",
            "{\"id\": \"dW09IT9URRp-tQiK_4vjr\", \"custom_id\": \"translation-0\", \"response\": null, \"error\": {\"code\": \"inference_failed\", \"message\": \"Maximum retries reached\"}}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Batch helper utilities\"\"\"\n",
        "\n",
        "import time, json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def wait_for_batch(client, batch_id: str, interval: int = 2) -> Any:\n",
        "    \"\"\"Poll the batch until it completes; returns final status object.\"\"\"\n",
        "    while True:\n",
        "        status = client.batches.retrieve(batch_id)\n",
        "        if status.status == \"completed\":\n",
        "            return status\n",
        "        print(\"Status:\", status.status)\n",
        "        time.sleep(interval)\n",
        "\n",
        "def ndjson_to_dicts(client, file_id: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Download a file and parse ND‚ÄëJSON into a list of dicts, skipping blanks.\"\"\"\n",
        "    text = client.files.content(file_id).text\n",
        "    return [json.loads(line) for line in text.splitlines() if line.strip()]\n",
        "\n",
        "def show_translations(docs: List[str], records: List[Dict[str, Any]]) -> None:\n",
        "    \"\"\"Pretty‚Äëprint originals next to their Spanish translations.\"\"\"\n",
        "    for i, original in enumerate(docs):\n",
        "        translated = records[i].get(\"response\") if i < len(records) else \"<missing>\"\n",
        "        print(f\"\\nOriginal: {original}\\nSpanish:  {translated}\")\n",
        "\n",
        "def show_errors(err_records: List[Dict[str, Any]], limit: int = 10) -> None:\n",
        "    \"\"\"Display up to *limit* raw error log lines.\"\"\"\n",
        "    print(\"‚ö†Ô∏è  Output empty ‚Äì showing error log lines:\\n\")\n",
        "    for rec in err_records[:limit]:\n",
        "        print(json.dumps(rec))\n",
        "\n",
        "# ‚îÄ‚îÄ Main workflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "status      = wait_for_batch(client, batch.id)\n",
        "output      = ndjson_to_dicts(client, status.output_file_id)\n",
        "error_lines = ndjson_to_dicts(client, getattr(status, \"error_file_id\", None)) if getattr(status, \"error_file_id\", None) else []\n",
        "\n",
        "if output:\n",
        "    show_translations(documents, output)\n",
        "else:\n",
        "    show_errors(error_lines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "The results come back as JSONL too, with each line containing the custom_id you specified and the translation. This makes it trivial to match translations back to your original documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Scaling to Millions of Documents\n",
        "\n",
        "The real power comes when you scale up. Let's say you're translating an entire e-commerce catalog with 1,000,000 products into 5 languages. That's 5,000,000 translations. Before LLMs, this would be a undoable number of translations. Now its trivial. Let's calcualtion the cost of this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation job size:\n",
            "  Languages: 5\n",
            "  Products: 1,000,000\n",
            "  Total requests: 5,000,000\n",
            "  Estimated tokens: 15,000,000,000\n",
            "  Estimated cost: $1500.00\n",
            "  Cost per translation: $0.0003\n"
          ]
        }
      ],
      "source": [
        "# Simulating a large catalog\n",
        "languages = [\"es\", \"fr\", \"de\", \"ja\", \"zh\"]\n",
        "products_per_language = 1000000\n",
        "total_requests = len(languages) * products_per_language\n",
        "\n",
        "# Estimate costs (using Llama 3.2 1B)\n",
        "avg_tokens_per_request = 3000  # ~1500 input, ~1500 output\n",
        "total_tokens = total_requests * avg_tokens_per_request\n",
        "cost_per_million_tokens = 0.10  # Check current pricing\n",
        "total_cost = (total_tokens / 1_000_000) * cost_per_million_tokens\n",
        "\n",
        "print(f\"Translation job size:\")\n",
        "print(f\"  Languages: {len(languages)}\")\n",
        "print(f\"  Products: {products_per_language:,}\")\n",
        "print(f\"  Total requests: {total_requests:,}\")\n",
        "print(f\"  Estimated tokens: {total_tokens:,}\")\n",
        "print(f\"  Estimated cost: ${total_cost:.2f}\")\n",
        "print(f\"  Cost per translation: ${total_cost/total_requests:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "At these scales, traditional translation APIs would either reject your requests or charge enterprise rates. With Inference.net's Batch API, you just upload larger JSONL files and wait. No rate limits, no throttling, just results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Production Tips\n",
        "\n",
        "Here's some general tips for translation:\n",
        "\n",
        "**1. Use the smallest model that works.** Llama 3.2 1B is good enough for most translation tasks, but struggles with following directions/processing long documents. Try different models and see what works well. If you are processing less than 100M tokens, just use an 8B model and call it a day.\n",
        "\n",
        "**2. Add glossaries to your system prompt.** If you have specific terms that must be translated consistently, you can inject glossaries into your prompt.\n",
        "\n",
        "**3. Break long texts up into parts, and then translate the parts and merge.** Don't try to translate very long documents (more than one or 1/2 page) at once. You may want to try breaking into paragraphs too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**4. Use webhooks for large jobs or with new data.** If you constantly have new data coming in, a single static batch job might make less sense than a processing pipeline where you send new texts as they come in, we process them and send the results to your webhook, which then updates your database. \n",
        "\n",
        "For more info on this, check out our docs on this:\n",
        "[Webhooks Quick Reference](https://docs.inference.net/features/asynchronous-inference/webhooks/quick-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**5. Validate critical translations.** For important content, run a second pass with a different model to check for issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Demonstrating translation validation:\n",
            "Original: Professional camera with 4K video recording\n",
            "Primary translation: C√°mara profesional con grabaci√≥n de video 4K\n",
            "\n",
            "Verifying translation quality...\n",
            "Is correct: True\n",
            "Confidence: 1.00\n",
            "Explanation: The translation is accurate. Both the English term 'Professional camera' and 'C√°mara profesional' as well as the technical term '4K video recording' and 'grabaci√≥n de video 4K' accurately and precisely convey the meaning and function of the original text. The translation maintains the exact details and context of the original.\n",
            "‚úÖ Translation verified as correct!\n"
          ]
        }
      ],
      "source": [
        "def translate_with_model(model_name, text, target_language):\n",
        "    \"\"\"Translate text using specified model\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Translate the following text to {target_language}. Preserve formatting and technical terms. Only translate the text, do not add any other text.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": text\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def verify_translation(original_text, translation, source_lang, target_lang, model_name):\n",
        "    \"\"\"Use a second model to verify if translation is correct, returns True/False\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You are a translation quality checker. Evaluate if the {target_lang} translation accurately represents the {source_lang} original text. Consider meaning, context, and technical accuracy. Respond in JSON format.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Original ({source_lang}): {original_text}\\nTranslation ({target_lang}): {translation}\\n\\nIs this translation correct?\"\n",
        "            }\n",
        "        ],\n",
        "        response_format={\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": {\n",
        "                \"name\": \"translation_verification\",\n",
        "                \"schema\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"is_correct\": {\"type\": \"boolean\"},\n",
        "                        \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
        "                        \"explanation\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"required\": [\"is_correct\", \"confidence\", \"explanation\"],\n",
        "                    \"additionalProperties\": False\n",
        "                },\n",
        "                \"strict\": True\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    import json\n",
        "    result = json.loads(response.choices[0].message.content)\n",
        "    return result\n",
        "\n",
        "def flag_for_human_review(text, original):\n",
        "    \"\"\"Flag text for human review\"\"\"\n",
        "    print(f\"Translation flagged for review:\")\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Translation: {text}\")\n",
        "\n",
        "# Create a mock product for demonstration\n",
        "class MockProduct:\n",
        "    def __init__(self, description, price, is_featured=False):\n",
        "        self.description = description\n",
        "        self.price = price\n",
        "        self.is_featured = is_featured\n",
        "\n",
        "# Example usage with a high-value product\n",
        "product = MockProduct(\"Professional camera with 4K video recording\", 1500, is_featured=True)\n",
        "target_language = \"Spanish\"\n",
        "\n",
        "print(\"Demonstrating translation validation:\")\n",
        "print(f\"Original: {product.description}\")\n",
        "\n",
        "# First pass: fast translation with 1B model\n",
        "primary_translation = translate_with_model(\"google/gemma-3-27b-instruct/bf-16\", product.description, target_language)\n",
        "print(f\"Primary translation: {primary_translation}\")\n",
        "\n",
        "# Second pass: verify translation quality with 3B model\n",
        "if product.is_featured or product.price > 1000:\n",
        "    print(\"Verifying translation quality...\")\n",
        "    verification_result = verify_translation(\n",
        "        original_text=product.description,\n",
        "        translation=primary_translation,\n",
        "        source_lang=\"English\",\n",
        "        target_lang=target_language,\n",
        "        model_name=\"qwen/qwen2.5-7b-instruct/bf-16\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Is correct: {verification_result['is_correct']}\")\n",
        "    print(f\"Confidence: {verification_result['confidence']:.2f}\")\n",
        "    print(f\"Explanation: {verification_result['explanation']}\")\n",
        "    \n",
        "    if not verification_result['is_correct']:\n",
        "        flag_for_human_review(primary_translation, product.description)\n",
        "    else:\n",
        "        print(\"‚úÖ Translation verified as correct!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Real-World Example: Chunking and Translating Documentation\n",
        "\n",
        "Let's fetch a real markdown document and translate it in chunks using the batch API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Users/michaelryaboy/recent-projects/inference-webhook/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Fetching content from https://ai-sdk.dev/llms.txt...\n",
            "Fetched 789804 characters\n",
            "First 200 characters:\n",
            "---\n",
            "title: RAG Chatbot\n",
            "description: Learn how to build a RAG Chatbot with the AI SDK and Next.js\n",
            "tags: ['rag', 'chatbot', 'next', 'embeddings', 'database', 'retrieval']\n",
            "---\n",
            "\n",
            "# RAG Chatbot Guide\n",
            "\n",
            "In th...\n"
          ]
        }
      ],
      "source": [
        "# Install chonkie for smart chunking\n",
        "%pip install chonkie requests -q\n",
        "\n",
        "import requests\n",
        "from chonkie import RecursiveChunker\n",
        "\n",
        "# Fetch the markdown content\n",
        "url = \"https://ai-sdk.dev/llms.txt\"\n",
        "print(f\"Fetching content from {url}...\")\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "markdown_content = response.text\n",
        "\n",
        "print(f\"Fetched {len(markdown_content)} characters\")\n",
        "print(\"First 200 characters:\")\n",
        "print(markdown_content[:200] + \"...\")\n",
        "\n",
        "# Initialize the markdown chunker\n",
        "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")\n",
        "\n",
        "# Chunk the content on markdown headers\n",
        "chunks = chunker.chunk(markdown_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunked into 740 sections:\n",
            "Chunk 1: 684 chars, level 0\n",
            "  Preview: ---\n",
            "title: RAG Chatbot\n",
            "description: Learn how to build a RAG Chatbot with the AI SDK and Next.js\n",
            "tag...\n",
            "\n",
            "Chunk 2: 1554 chars, level 0\n",
            "  Preview: ### Why is RAG important?\n",
            "\n",
            "While LLMs are powerful, the information they can reason on is restricted...\n",
            "\n",
            "Chunk 3: 1988 chars, level 0\n",
            "  Preview: ### Embedding\n",
            "\n",
            "[Embeddings](/docs/ai-sdk-core/embeddings) are a way to represent words, phrases, or...\n",
            "\n",
            "Chunk 4: 1437 chars, level 0\n",
            "  Preview: ### All Together Now\n",
            "\n",
            "Combining all of this together, RAG is the process of enabling the model to re...\n",
            "\n",
            "Chunk 5: 1099 chars, level 0\n",
            "  Preview: ### Clone Repo\n",
            "\n",
            "To reduce the scope of this guide, you will be starting with a [repository](https://...\n",
            "\n",
            "Total chunks to translate: 740\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nChunked into {len(chunks)} sections:\")\n",
        "for i, chunk in enumerate(chunks[:5]):  # Show first 5 chunks\n",
        "    print(f\"Chunk {i+1}: {len(chunk.text)} chars, level {chunk.level}\")\n",
        "    print(f\"  Preview: {chunk.text[:100].strip()}...\")\n",
        "    print()\n",
        "\n",
        "print(f\"Total chunks to translate: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating batch translation requests...\n",
            "Created 1480 translation requests\n",
            "Languages: ['Spanish', 'French']\n",
            "Chunks per language: 740\n",
            "Number of chunks with 0 chars: 0\n",
            "Number of chunks with 100 chars: 248\n",
            "Number of chunks with 200 chars: 602\n",
            "Number of chunks with 300 chars: 734\n",
            "Number of chunks with 400 chars: 740\n",
            "Number of chunks with 500 chars: 740\n",
            "Number of chunks with 600 chars: 740\n",
            "Number of chunks with 700 chars: 740\n",
            "Number of chunks with 800 chars: 740\n",
            "Number of chunks with 900 chars: 740\n"
          ]
        }
      ],
      "source": [
        "# Create batch translation requests for all chunks\n",
        "target_languages = [\"Spanish\", \"French\"]\n",
        "\n",
        "print(\"Creating batch translation requests...\")\n",
        "\n",
        "# Prepare batch requests for all chunks and languages\n",
        "batch_requests = []\n",
        "for lang in target_languages:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        request = {\n",
        "            \"custom_id\": f\"chunk-{i}-{lang.lower()}\",\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"mistralai/mistral-nemo-12b-instruct/fp-8\",\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": f\"\"\"Translate this technical documentation chunk to {lang}.\n",
        "\n",
        "Rules:\n",
        "- Preserve ALL markdown formatting (headers, links, code blocks, etc.)\n",
        "- Keep code examples in English\n",
        "- Preserve technical terms when appropriate\n",
        "- Maintain the structure and meaning\n",
        "- Only translate the content, don't add explanations\n",
        "- ONLY give me the translated version of the content, no other text\n",
        "- Code stays exactly as it is\n",
        "\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": chunk.text\n",
        "                    }\n",
        "                ],\n",
        "                \"max_tokens\": len(chunk.text.split()) * 10,  # Generous token limit\n",
        "                \"temperature\": 0.3\n",
        "            }\n",
        "        }\n",
        "        batch_requests.append(request)\n",
        "\n",
        "print(f\"Created {len(batch_requests)} translation requests\")\n",
        "print(f\"Languages: {target_languages}\")\n",
        "print(f\"Chunks per language: {len(chunks)}\")\n",
        "\n",
        "# print number of chunks at each 100 character interval\n",
        "for i in range(0, 1000, 100):\n",
        "    print(f\"Number of chunks with {i} chars: {len([chunk for chunk in chunks if len(chunk.text.split()) <= i])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch file created: docs_chunks_translation_1752537608.jsonl\n",
            "Uploading batch file...\n",
            "Batch translation job started: zwg5rS3WgDZqvG-va65Bl\n",
            "This will translate all chunks into multiple languages simultaneously!\n"
          ]
        }
      ],
      "source": [
        "# Write batch file\n",
        "batch_filename = f\"docs_chunks_translation_{int(time.time())}.jsonl\"\n",
        "with open(batch_filename, \"w\") as f:\n",
        "    for req in batch_requests:\n",
        "        f.write(json.dumps(req) + \"\\n\")\n",
        "\n",
        "print(f\"Batch file created: {batch_filename}\")\n",
        "\n",
        "# Upload and start batch job\n",
        "print(\"Uploading batch file...\")\n",
        "with open(batch_filename, \"rb\") as f:\n",
        "    batch_file = client.files.create(\n",
        "        file=f,\n",
        "        purpose=\"batch\"\n",
        "    )\n",
        "\n",
        "# Create the batch job\n",
        "batch_job = client.batches.create(\n",
        "    input_file_id=batch_file.id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"job_type\": \"documentation_translation\",\n",
        "        \"source_url\": url,\n",
        "        \"languages\": \",\".join(target_languages),\n",
        "        \"total_chunks\": str(len(chunks))\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Batch translation job started: {batch_job.id}\")\n",
        "print(\"This will translate all chunks into multiple languages simultaneously!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking batch status...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'batch_job' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Wait for completion \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     batch_status \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mretrieve(\u001b[43mbatch_job\u001b[49m\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_status\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_status\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_job' is not defined"
          ]
        }
      ],
      "source": [
        "# Check batch status and get results\n",
        "print(\"Checking batch status...\")\n",
        "\n",
        "# Wait for completion \n",
        "while True:\n",
        "    batch_status = client.batches.retrieve(batch_job.id)\n",
        "    print(f\"Status: {batch_status.status}\")\n",
        "    \n",
        "    if batch_status.status == \"completed\":\n",
        "        print(\"‚úÖ Batch completed!\")\n",
        "        break\n",
        "    elif batch_status.status == \"failed\":\n",
        "        print(\"‚ùå Batch failed!\")\n",
        "        break\n",
        "    elif batch_status.status in [\"cancelled\", \"expired\"]:\n",
        "        print(f\"‚ùå Batch {batch_status.status}!\")\n",
        "        break\n",
        "    \n",
        "    time.sleep(3)\n",
        "\n",
        "if batch_status.status == \"completed\":\n",
        "    # Download and parse results\n",
        "    print(\"Downloading results...\")\n",
        "    results_file = client.files.content(batch_status.output_file_id)\n",
        "\n",
        "    error_file = client.files.content(batch_status.error_file_id)\n",
        "    print(error_file.text)\n",
        "    \n",
        "    # Parse all translation results\n",
        "    translations = {}\n",
        "    for line in results_file.text.strip().split('\\n'):\n",
        "        if line.strip():\n",
        "            result = json.loads(line)\n",
        "            custom_id = result['custom_id']\n",
        "            translation = result['response']['body']['choices'][0]['message']['content']\n",
        "            translations[custom_id] = translation\n",
        "    \n",
        "    print(f\"Received {len(translations)} translations\")\n",
        "    \n",
        "    # Reconstruct documents by language\n",
        "    reconstructed_docs = {}\n",
        "    \n",
        "    for lang in target_languages:\n",
        "        print(f\"\\nüìÑ Reconstructing {lang} document...\")\n",
        "        lang_key = lang.lower()\n",
        "        \n",
        "        # Get all chunks for this language, sorted by chunk number\n",
        "        lang_chunks = []\n",
        "        for i in range(len(chunks)):\n",
        "            chunk_id = f\"chunk-{i}-{lang_key}\"\n",
        "            if chunk_id in translations:\n",
        "                lang_chunks.append((i, translations[chunk_id]))\n",
        "        \n",
        "        # Sort by chunk index and concatenate\n",
        "        lang_chunks.sort(key=lambda x: x[0])\n",
        "        reconstructed_text = '\\n\\n'.join([chunk_text for _, chunk_text in lang_chunks])\n",
        "        reconstructed_docs[lang] = reconstructed_text\n",
        "        \n",
        "        print(f\"‚úÖ {lang} document reconstructed: {len(reconstructed_text)} characters\")\n",
        "    \n",
        "    # Show samples from each language\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRANSLATION RESULTS PREVIEW\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for lang, doc in reconstructed_docs.items():\n",
        "        print(f\"\\nüåç {lang.upper()} VERSION:\")\n",
        "        print(\"-\" * 40)\n",
        "        # Show first 500 characters\n",
        "        preview = doc[:500].strip()\n",
        "        print(preview)\n",
        "        if len(doc) > 500:\n",
        "            print(\"...\\n[Truncated - full translation available]\")\n",
        "        print()\n",
        "    \n",
        "    # Save translated documents to files\n",
        "    print(\"üíæ Saving translated documents...\")\n",
        "    for lang, doc in reconstructed_docs.items():\n",
        "        filename = f\"ai-sdk-llms_{lang.lower()}.md\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(doc)\n",
        "        print(f\"Saved: {filename}\")\n",
        "    \n",
        "    print(\"\\nüéâ Translation complete! All documents have been translated and saved.\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Could not retrieve results - batch did not complete successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "We did it! Our results are saved in two files:\n",
        "ai-sdk-llms_spanish.md and Saved: ai-sdk-llms_french.md.\n",
        "\n",
        "We just executed the following workflow:\n",
        "\n",
        "1. **Smart Chunking**: Using Chonkie's markdown recipe to intelligently split documentation on headers\n",
        "2. **Batch Processing**: Creating hundreds of translation requests simultaneously \n",
        "3. **Multi-language**: Translating to multiple target languages in a single batch job\n",
        "\n",
        "This approach scales beautifully - whether you're translating a few pages or an entire documentation site with tens of thousands of pages.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
