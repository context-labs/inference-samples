{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LLM Translation at Scale with Inference.net Batch API\n",
        "\n",
        "LLMs are remarkably good at translation. It doesn't take a particularly strong LLM to perform most translations: a small 8-30B parameter model is more than strong enough for translating between most languages. The OpenRouter leaderboard shows the most popular models used for translation are tiny and fast--which allow you to translate very large amounts of text remarkably cheaply.\n",
        "\n",
        "![image](openrouter.png)\n",
        "\n",
        "For most translation tasks, the specific cheap model/provider you use isn't particularly important. But some translation jobs scale to the hundreds of millions to tens of trillions of tokens, and at that point price and rate limits become a factor. \n",
        "\n",
        "This is where Inference excels: we serve models extremely cheaply and have no rate limits for time-insensitive batch jobs like this.\n",
        "\n",
        "Here's how you can get started with LLM translation:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setting Up Your Translation Pipeline\n",
        "\n",
        "The beauty of using Inference.net is that it's compatible with the OpenAI SDK, so you can get started in seconds. Just point the client at our batch endpoint:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai -q\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://batch.inference.net/v1\",\n",
        "    api_key=os.getenv(\"INFERENCE_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Your First Translation Batch\n",
        "\n",
        "Let's say you need to translate a batch of product descriptions. With the Batch API, you prepare all your requests in a JSONL file where each line is a complete translation request:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created batch file with 5 translation requests\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Some product descriptions to translate\n",
        "documents = [\n",
        "    \"High-quality wireless headphones with noise cancellation\",\n",
        "    \"Ergonomic office chair with lumbar support\", \n",
        "    \"Smart home thermostat with energy-saving features\",\n",
        "    \"Professional camera with 4K video recording\",\n",
        "    \"Portable power bank with fast charging\"\n",
        "]\n",
        "\n",
        "# Create the batch file\n",
        "with open(\"translation_batch.jsonl\", \"w\") as f:\n",
        "    for idx, doc in enumerate(documents):\n",
        "        request = {\n",
        "            \"custom_id\": f\"translation-{idx}\",\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"meta-llama/llama-3.2-1b-instruct/fp-8\",\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a professional translator. Translate the following English text to Spanish, preserving the tone and meaning. Only translate the text, do not add any other text.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": doc\n",
        "                    }\n",
        "                ],\n",
        "                \"max_tokens\": 100,\n",
        "                \"temperature\": 0.3  # Lower temperature for consistent translations\n",
        "            }\n",
        "        }\n",
        "        f.write(json.dumps(request) + \"\\n\")\n",
        "\n",
        "print(f\"Created batch file with {len(documents)} translation requests\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "The key here is that each request is self-contained. You specify the model, the system prompt (your translation instructions), and the text to translate. Setting temperature to 0.3 gives you consistent, professional translations without too much creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Launching the Job\n",
        "\n",
        "Once your batch file is ready, it's a two-step process: upload the file, then create the batch job.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch job started: hCSfG2_fd7FCwACWDevSv\n"
          ]
        }
      ],
      "source": [
        "# Upload the file\n",
        "with open(\"translation_batch.jsonl\", \"rb\") as f:\n",
        "    batch_file = client.files.create(\n",
        "        file=f,\n",
        "        purpose=\"batch\"\n",
        "    )\n",
        "\n",
        "# Create the batch job\n",
        "batch = client.batches.create(\n",
        "    input_file_id=batch_file.id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"job_type\": \"translation\",\n",
        "        \"language_pair\": \"en-es\",\n",
        "        \"document_count\": str(len(documents))\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Batch job started: {batch.id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "The batch starts processing immediately. For small jobs like this, it'll complete in seconds. For massive jobs with millions of tokens, it might take a few hours--but that's still faster than hitting rate limits with synchronous APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Getting Your Translations\n",
        "\n",
        "Once the batch completes, you download the results and parse them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: in_progress\n",
            "\n",
            "Original: High-quality wireless headphones with noise cancellation\n",
            "Spanish: Altas calidades de auriculares inal√°mbricos con reducci√≥n de ruido de fondo\n",
            "\n",
            "Original: Ergonomic office chair with lumbar support\n",
            "Spanish: Asiento ergon√≥mico con soporte lumbar.\n",
            "\n",
            "Original: Smart home thermostat with energy-saving features\n",
            "Spanish: Tecnolog√≠a de habitaci√≥n inteligente con caracter√≠sticas de ahorro de energ√≠a.\n",
            "\n",
            "Original: Professional camera with 4K video recording\n",
            "Spanish: Una c√°mara profesional con grabaci√≥n de video de 4K.\n",
            "\n",
            "Original: Portable power bank with fast charging\n",
            "Spanish: Port√°til cargador de bater√≠a port√°til con carga r√°pida.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    batch_status = client.batches.retrieve(batch.id)\n",
        "    if batch_status.status == \"completed\":\n",
        "        break\n",
        "    print(f\"Status: {batch_status.status}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# Download and parse results\n",
        "results_content = client.files.content(batch_status.output_file_id)\n",
        "translations = {}\n",
        "\n",
        "for line in results_content.text.strip().split('\\n'):\n",
        "    result = json.loads(line)\n",
        "    custom_id = result['custom_id']\n",
        "    translation = result['response']['body']['choices'][0]['message']['content']\n",
        "    translations[custom_id] = translation\n",
        "\n",
        "# Show the translations\n",
        "for idx, original in enumerate(documents):\n",
        "    print(f\"\\nOriginal: {original}\")\n",
        "    print(f\"Spanish: {translations[f'translation-{idx}']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "The results come back as JSONL too, with each line containing the custom_id you specified and the translation. This makes it trivial to match translations back to your original documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Scaling to Millions of Documents\n",
        "\n",
        "The real power comes when you scale up. Let's say you're translating an entire e-commerce catalog with 1,000,000 products into 5 languages. That's 5,000,000 translations. Before LLMs, this would be a undoable number of translations. Now its trivial. Let's calcualtion the cost of this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation job size:\n",
            "  Languages: 5\n",
            "  Products: 1,000,000\n",
            "  Total requests: 5,000,000\n",
            "  Estimated tokens: 15,000,000,000\n",
            "  Estimated cost: $1500.00\n",
            "  Cost per translation: $0.0003\n"
          ]
        }
      ],
      "source": [
        "# Simulating a large catalog\n",
        "languages = [\"es\", \"fr\", \"de\", \"ja\", \"zh\"]\n",
        "products_per_language = 1000000\n",
        "total_requests = len(languages) * products_per_language\n",
        "\n",
        "# Estimate costs (using Llama 3.2 1B)\n",
        "avg_tokens_per_request = 3000  # ~1500 input, ~1500 output\n",
        "total_tokens = total_requests * avg_tokens_per_request\n",
        "cost_per_million_tokens = 0.10  # Check current pricing\n",
        "total_cost = (total_tokens / 1_000_000) * cost_per_million_tokens\n",
        "\n",
        "print(f\"Translation job size:\")\n",
        "print(f\"  Languages: {len(languages)}\")\n",
        "print(f\"  Products: {products_per_language:,}\")\n",
        "print(f\"  Total requests: {total_requests:,}\")\n",
        "print(f\"  Estimated tokens: {total_tokens:,}\")\n",
        "print(f\"  Estimated cost: ${total_cost:.2f}\")\n",
        "print(f\"  Cost per translation: ${total_cost/total_requests:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "At these scales, traditional translation APIs would either reject your requests or charge enterprise rates. With Inference.net's Batch API, you just upload larger JSONL files and wait. No rate limits, no throttling, just results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Production Tips\n",
        "\n",
        "Here's some general tips for translation:\n",
        "\n",
        "**1. Use the smallest model that works.** Llama 3.2 1B is good enough for most translation tasks, but struggles with following directions/processing long documents. Try different models and see what works well. If you are processing less than 100M tokens, just use an 8B model and call it a day.\n",
        "\n",
        "**2. Add glossaries to your system prompt.** If you have specific terms that must be translated consistently, you can inject glossaries into your prompt.\n",
        "\n",
        "**3. Break long texts up into parts, and then translate the parts and merge.** Don't try to translate very long documents (more than one or 1/2 page) at once. You may want to try breaking into paragraphs too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**4. Use webhooks for large jobs or with new data.** If you constantly have new data coming in, a single static batch job might make less sense than a processing pipeline where you send new texts as they come in, we process them and send the results to your webhook, which then updates your database. \n",
        "\n",
        "For more info on this, check out our docs on this:\n",
        "[Webhooks Quick Reference](https://docs.inference.net/features/asynchronous-inference/webhooks/quick-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**5. Validate critical translations.** For important content, run a second pass with a different model to check for issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Demonstrating translation validation:\n",
            "Original: Professional camera with 4K video recording\n",
            "Primary translation: C√°mara profesional con grabaci√≥n de video 4K\n",
            "Verifying translation quality...\n",
            "Is correct: True\n",
            "Confidence: 1.00\n",
            "Explanation: The translation accurately conveys the meaning of the original English text. Both the term 'camara profesional' and 'grabacion de video 4K' correctly represent 'professional camera with 4K video recording'. There is no loss of information or contextual misrepresentation.\n",
            "‚úÖ Translation verified as correct!\n"
          ]
        }
      ],
      "source": [
        "def translate_with_model(model_name, text, target_language):\n",
        "    \"\"\"Translate text using specified model\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Translate the following text to {target_language}. Preserve formatting and technical terms. Only translate the text, do not add any other text.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": text\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def verify_translation(original_text, translation, source_lang, target_lang, model_name):\n",
        "    \"\"\"Use a second model to verify if translation is correct, returns True/False\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You are a translation quality checker. Evaluate if the {target_lang} translation accurately represents the {source_lang} original text. Consider meaning, context, and technical accuracy. Respond in JSON format.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Original ({source_lang}): {original_text}\\nTranslation ({target_lang}): {translation}\\n\\nIs this translation correct?\"\n",
        "            }\n",
        "        ],\n",
        "        response_format={\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": {\n",
        "                \"name\": \"translation_verification\",\n",
        "                \"schema\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"is_correct\": {\"type\": \"boolean\"},\n",
        "                        \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
        "                        \"explanation\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"required\": [\"is_correct\", \"confidence\", \"explanation\"],\n",
        "                    \"additionalProperties\": False\n",
        "                },\n",
        "                \"strict\": True\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    import json\n",
        "    result = json.loads(response.choices[0].message.content)\n",
        "    return result\n",
        "\n",
        "def flag_for_human_review(text, original):\n",
        "    \"\"\"Flag text for human review\"\"\"\n",
        "    print(f\"Translation flagged for review:\")\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Translation: {text}\")\n",
        "\n",
        "# Create a mock product for demonstration\n",
        "class MockProduct:\n",
        "    def __init__(self, description, price, is_featured=False):\n",
        "        self.description = description\n",
        "        self.price = price\n",
        "        self.is_featured = is_featured\n",
        "\n",
        "# Example usage with a high-value product\n",
        "product = MockProduct(\"Professional camera with 4K video recording\", 1500, is_featured=True)\n",
        "target_language = \"Spanish\"\n",
        "\n",
        "print(\"Demonstrating translation validation:\")\n",
        "print(f\"Original: {product.description}\")\n",
        "\n",
        "# First pass: fast translation with 1B model\n",
        "primary_translation = translate_with_model(\"google/gemma-3-27b-instruct/bf-16\", product.description, target_language)\n",
        "print(f\"Primary translation: {primary_translation}\")\n",
        "\n",
        "# Second pass: verify translation quality with 3B model\n",
        "if product.is_featured or product.price > 1000:\n",
        "    print(\"Verifying translation quality...\")\n",
        "    verification_result = verify_translation(\n",
        "        original_text=product.description,\n",
        "        translation=primary_translation,\n",
        "        source_lang=\"English\",\n",
        "        target_lang=target_language,\n",
        "        model_name=\"qwen/qwen2.5-7b-instruct/bf-16\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Is correct: {verification_result['is_correct']}\")\n",
        "    print(f\"Confidence: {verification_result['confidence']:.2f}\")\n",
        "    print(f\"Explanation: {verification_result['explanation']}\")\n",
        "    \n",
        "    if not verification_result['is_correct']:\n",
        "        flag_for_human_review(primary_translation, product.description)\n",
        "    else:\n",
        "        print(\"‚úÖ Translation verified as correct!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Real-World Example: Chunking and Translating Documentation\n",
        "\n",
        "Let's fetch a real markdown document and translate it in chunks using the batch API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install chonkie for smart chunking\n",
        "%pip install chonkie requests -q\n",
        "\n",
        "import requests\n",
        "from chonkie import RecursiveChunker\n",
        "\n",
        "# Fetch the markdown content\n",
        "url = \"https://ai-sdk.dev/llms.txt\"\n",
        "print(f\"Fetching content from {url}...\")\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "markdown_content = response.text\n",
        "\n",
        "print(f\"Fetched {len(markdown_content)} characters\")\n",
        "print(\"First 200 characters:\")\n",
        "print(markdown_content[:200] + \"...\")\n",
        "\n",
        "# Initialize the markdown chunker\n",
        "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")\n",
        "\n",
        "# Chunk the content on markdown headers\n",
        "chunks = chunker.chunk(markdown_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunked into 574 sections:\n",
            "Chunk 1: 1183 chars, level 0\n",
            "  Preview: ---\n",
            "title: Node.js HTTP Server\n",
            "description: Learn how to use the AI SDK in a Node.js HTTP server\n",
            "tag...\n",
            "\n",
            "Chunk 2: 1439 chars, level 0\n",
            "  Preview: ### Sending Custom Data\n",
            "\n",
            "`pipeDataStreamToResponse` can be used to send custom data to the client....\n",
            "\n",
            "Chunk 3: 1420 chars, level 0\n",
            "  Preview: ## Troubleshooting\n",
            "\n",
            "- Streaming not working when [proxied](/docs/troubleshooting/streaming-not-worki...\n",
            "\n",
            "Chunk 4: 1126 chars, level 0\n",
            "  Preview: ### Sending Custom Data\n",
            "\n",
            "`pipeDataStreamToResponse` can be used to send custom data to the client....\n",
            "\n",
            "Chunk 5: 1388 chars, level 0\n",
            "  Preview: ### Text Stream\n",
            "\n",
            "You can send a text stream to the client using `pipeTextStreamToResponse`.\n",
            "\n",
            "```ts f...\n",
            "\n",
            "Total chunks to translate: 574\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nChunked into {len(chunks)} sections:\")\n",
        "for i, chunk in enumerate(chunks[:5]):  # Show first 5 chunks\n",
        "    print(f\"Chunk {i+1}: {len(chunk.text)} chars, level {chunk.level}\")\n",
        "    print(f\"  Preview: {chunk.text[:100].strip()}...\")\n",
        "    print()\n",
        "\n",
        "print(f\"Total chunks to translate: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating batch translation requests...\n",
            "Created 1148 translation requests\n",
            "Languages: ['Spanish', 'French']\n",
            "Chunks per language: 574\n",
            "Batch file created: docs_chunks_translation_1749757657.jsonl\n",
            "Uploading batch file...\n",
            "Batch translation job started: AtrMeGqY6aXmdhT3uoqhU\n",
            "This will translate all chunks into multiple languages simultaneously!\n"
          ]
        }
      ],
      "source": [
        "# Create batch translation requests for all chunks\n",
        "target_languages = [\"Spanish\", \"French\"]\n",
        "\n",
        "print(\"Creating batch translation requests...\")\n",
        "\n",
        "# Prepare batch requests for all chunks and languages\n",
        "batch_requests = []\n",
        "for lang in target_languages:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        request = {\n",
        "            \"custom_id\": f\"chunk-{i}-{lang.lower()}\",\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct/fp-8\",\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": f\"\"\"Translate this technical documentation chunk to {lang}.\n",
        "\n",
        "Rules:\n",
        "- Preserve ALL markdown formatting (headers, links, code blocks, etc.)\n",
        "- Keep code examples in English\n",
        "- Preserve technical terms when appropriate\n",
        "- Maintain the structure and meaning\n",
        "- Only translate the content, don't add explanations\n",
        "- ONLY give me the translated version of the content, no other text\n",
        "- Code stays exactly as it is\n",
        "\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": chunk.text\n",
        "                    }\n",
        "                ],\n",
        "                \"max_tokens\": len(chunk.text.split()) * 3,  # Generous token limit\n",
        "                \"temperature\": 0.3\n",
        "            }\n",
        "        }\n",
        "        batch_requests.append(request)\n",
        "\n",
        "print(f\"Created {len(batch_requests)} translation requests\")\n",
        "print(f\"Languages: {target_languages}\")\n",
        "print(f\"Chunks per language: {len(chunks)}\")\n",
        "\n",
        "# Write batch file\n",
        "batch_filename = f\"docs_chunks_translation_{int(time.time())}.jsonl\"\n",
        "with open(batch_filename, \"w\") as f:\n",
        "    for req in batch_requests:\n",
        "        f.write(json.dumps(req) + \"\\n\")\n",
        "\n",
        "print(f\"Batch file created: {batch_filename}\")\n",
        "\n",
        "# Upload and start batch job\n",
        "print(\"Uploading batch file...\")\n",
        "with open(batch_filename, \"rb\") as f:\n",
        "    batch_file = client.files.create(\n",
        "        file=f,\n",
        "        purpose=\"batch\"\n",
        "    )\n",
        "\n",
        "# Create the batch job\n",
        "batch_job = client.batches.create(\n",
        "    input_file_id=batch_file.id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"job_type\": \"documentation_translation\",\n",
        "        \"source_url\": url,\n",
        "        \"languages\": \",\".join(target_languages),\n",
        "        \"total_chunks\": str(len(chunks))\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Batch translation job started: {batch_job.id}\")\n",
        "print(\"This will translate all chunks into multiple languages simultaneously!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking batch status...\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: completed\n",
            "‚úÖ Batch completed!\n",
            "Downloading results...\n",
            "Received 1148 translations\n",
            "\n",
            "üìÑ Reconstructing Spanish document...\n",
            "‚úÖ Spanish document reconstructed: 640029 characters\n",
            "\n",
            "üìÑ Reconstructing French document...\n",
            "‚úÖ French document reconstructed: 652281 characters\n",
            "\n",
            "============================================================\n",
            "TRANSLATION RESULTS PREVIEW\n",
            "============================================================\n",
            "\n",
            "üåç SPANISH VERSION:\n",
            "----------------------------------------\n",
            "---\n",
            "title: Servidor HTTP de Node.js\n",
            "description: Aprende a utilizar el SDK de IA en un servidor HTTP de Node.js\n",
            "tags: ['servidores de API', 'transmisi√≥n en streaming']\n",
            "---\n",
            "\n",
            "# Servidor HTTP de Node.js\n",
            "\n",
            "Puedes utilizar el SDK de IA en un servidor HTTP de Node.js para generar texto y transmitirlo al cliente.\n",
            "\n",
            "## Ejemplos\n",
            "\n",
            "Los ejemplos arrancan un servidor HTTP simple que escucha en el puerto 8080. Puedes probarlo utilizando `curl`:\n",
            "\n",
            "```bash\n",
            "curl -X POST http://localhost:8080\n",
            "```\n",
            "\n",
            "<Nota>\n",
            "  Los ejemp\n",
            "...\n",
            "[Truncated - full translation available]\n",
            "\n",
            "\n",
            "üåç FRENCH VERSION:\n",
            "----------------------------------------\n",
            "---\n",
            "titre : Serveur HTTP Node.js\n",
            "description : Apprenez √† utiliser le SDK AI dans un serveur HTTP Node.js\n",
            "tags : ['serveurs API', 'streaming']\n",
            "---\n",
            "\n",
            "# Serveur HTTP Node.js\n",
            "\n",
            "Vous pouvez utiliser le SDK AI dans un serveur HTTP Node.js pour g√©n√©rer du texte et le transmettre au client.\n",
            "\n",
            "## Exemples\n",
            "\n",
            "Les exemples d√©marreront un serveur HTTP simple qui √©coute sur le port 8080. Vous pouvez par exemple le tester en utilisant `curl`¬†:\n",
            "\n",
            "```bash\n",
            "curl -X POST http://localhost:8080\n",
            "```\n",
            "\n",
            "<Note>\n",
            "  Les exemples\n",
            "...\n",
            "[Truncated - full translation available]\n",
            "\n",
            "üíæ Saving translated documents...\n",
            "Saved: ai-sdk-llms_spanish.md\n",
            "Saved: ai-sdk-llms_french.md\n",
            "\n",
            "üéâ Translation complete! All documents have been translated and saved.\n"
          ]
        }
      ],
      "source": [
        "# Check batch status and get results\n",
        "print(\"Checking batch status...\")\n",
        "\n",
        "# Wait for completion \n",
        "while True:\n",
        "    batch_status = client.batches.retrieve(batch_job.id)\n",
        "    print(f\"Status: {batch_status.status}\")\n",
        "    \n",
        "    if batch_status.status == \"completed\":\n",
        "        print(\"‚úÖ Batch completed!\")\n",
        "        break\n",
        "    elif batch_status.status == \"failed\":\n",
        "        print(\"‚ùå Batch failed!\")\n",
        "        break\n",
        "    elif batch_status.status in [\"cancelled\", \"expired\"]:\n",
        "        print(f\"‚ùå Batch {batch_status.status}!\")\n",
        "        break\n",
        "    \n",
        "    time.sleep(3)\n",
        "\n",
        "if batch_status.status == \"completed\":\n",
        "    # Download and parse results\n",
        "    print(\"Downloading results...\")\n",
        "    results_file = client.files.content(batch_status.output_file_id)\n",
        "    \n",
        "    # Parse all translation results\n",
        "    translations = {}\n",
        "    for line in results_file.text.strip().split('\\n'):\n",
        "        if line.strip():\n",
        "            result = json.loads(line)\n",
        "            custom_id = result['custom_id']\n",
        "            translation = result['response']['body']['choices'][0]['message']['content']\n",
        "            translations[custom_id] = translation\n",
        "    \n",
        "    print(f\"Received {len(translations)} translations\")\n",
        "    \n",
        "    # Reconstruct documents by language\n",
        "    reconstructed_docs = {}\n",
        "    \n",
        "    for lang in target_languages:\n",
        "        print(f\"\\nüìÑ Reconstructing {lang} document...\")\n",
        "        lang_key = lang.lower()\n",
        "        \n",
        "        # Get all chunks for this language, sorted by chunk number\n",
        "        lang_chunks = []\n",
        "        for i in range(len(chunks)):\n",
        "            chunk_id = f\"chunk-{i}-{lang_key}\"\n",
        "            if chunk_id in translations:\n",
        "                lang_chunks.append((i, translations[chunk_id]))\n",
        "        \n",
        "        # Sort by chunk index and concatenate\n",
        "        lang_chunks.sort(key=lambda x: x[0])\n",
        "        reconstructed_text = '\\n\\n'.join([chunk_text for _, chunk_text in lang_chunks])\n",
        "        reconstructed_docs[lang] = reconstructed_text\n",
        "        \n",
        "        print(f\"‚úÖ {lang} document reconstructed: {len(reconstructed_text)} characters\")\n",
        "    \n",
        "    # Show samples from each language\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRANSLATION RESULTS PREVIEW\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for lang, doc in reconstructed_docs.items():\n",
        "        print(f\"\\nüåç {lang.upper()} VERSION:\")\n",
        "        print(\"-\" * 40)\n",
        "        # Show first 500 characters\n",
        "        preview = doc[:500].strip()\n",
        "        print(preview)\n",
        "        if len(doc) > 500:\n",
        "            print(\"...\\n[Truncated - full translation available]\")\n",
        "        print()\n",
        "    \n",
        "    # Save translated documents to files\n",
        "    print(\"üíæ Saving translated documents...\")\n",
        "    for lang, doc in reconstructed_docs.items():\n",
        "        filename = f\"ai-sdk-llms_{lang.lower()}.md\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(doc)\n",
        "        print(f\"Saved: {filename}\")\n",
        "    \n",
        "    print(\"\\nüéâ Translation complete! All documents have been translated and saved.\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Could not retrieve results - batch did not complete successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "We did it! Our results are saved in two files:\n",
        "ai-sdk-llms_spanish.md and Saved: ai-sdk-llms_french.md.\n",
        "\n",
        "We just executed the following workflow:\n",
        "\n",
        "1. **Smart Chunking**: Using Chonkie's markdown recipe to intelligently split documentation on headers\n",
        "2. **Batch Processing**: Creating hundreds of translation requests simultaneously \n",
        "3. **Multi-language**: Translating to multiple target languages in a single batch job\n",
        "\n",
        "This approach scales beautifully - whether you're translating a few pages or an entire documentation site with tens of thousands of pages.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
