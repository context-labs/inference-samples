{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05474810",
   "metadata": {},
   "source": [
    "## RAG with ChromaDB, Chonkie, and Paul Graham's Essays\n",
    "\n",
    "Welcome! In this notebook, we’re building a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "\n",
    "Here’s what we’ll use:\n",
    "- ChromaDB for fast vector search\n",
    "- Chonkie for smart, semantic text chunking\n",
    "- OpenAI for embeddings and LLM completions\n",
    "- A dataset of Paul Graham’s essays (because they’re awesome)\n",
    "\n",
    "What you’ll learn:\n",
    "1. How to load and peek at a real-world essay dataset\n",
    "2. How to chunk text in a way that actually makes sense for retrieval\n",
    "3. How to store and search those chunks in a vector DB\n",
    "4. How to use retrieved context to make your LLM answers way better\n",
    "\n",
    "Let’s dive in 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19986a",
   "metadata": {},
   "source": [
    "### 1. Install dependencies and load data\n",
    "We’ll need the OpenAI-compatible SDK to interact with Inference.net, Chroma for vector storage, the `datasets` library for fetching data, and **Chonkie** for smart chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai chromadb datasets chonkie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311775c",
   "metadata": {},
   "source": [
    "### 2. Import Dependencies and Load the Paul Graham essays dataset\n",
    "\n",
    "We pull the full set of essays from 🤗 Hub, convert it to a Pandas DataFrame, and do a quick sanity check on the row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7eb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>|  \\n  \\nFebruary 2009  \\n  \\nHacker News was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>|  \\n  \\nMay 2008  \\n  \\nAdults lie constantly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|  \\n  \\nNovember 2008  \\n  \\nOne of the diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>|  \\n  \\nDecember 2010  \\n  \\nI was thinking r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|  \\n  \\n|  **Want to start a startup?** Get f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  |  \\n  \\nFebruary 2009  \\n  \\nHacker News was ...\n",
       "1  |  \\n  \\nMay 2008  \\n  \\nAdults lie constantly...\n",
       "2  |  \\n  \\nNovember 2008  \\n  \\nOne of the diffe...\n",
       "3  |  \\n  \\nDecember 2010  \\n  \\nI was thinking r...\n",
       "4  |  \\n  \\n|  **Want to start a startup?** Get f..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "from chonkie import SemanticChunker\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"pookie3000/paul_graham_all_essays\")\n",
    "ds = ds[\"train\"].to_pandas()\n",
    "print(len(ds))\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2af77",
   "metadata": {},
   "source": [
    "### 3. Chunk the essays semantically  \n",
    "`SemanticChunker` splits each essay into overlapping, semantically-coherent chunks—useful when you want to do retrieval at paragraph-level rather than whole-essay-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da3a8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 222/222 docs chunked [00:02<00:00, 92.83doc/s] 🌱 \n"
     ]
    }
   ],
   "source": [
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.47,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=5000,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")\n",
    "\n",
    "\n",
    "batch_chunks = chunker.chunk_batch(ds[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0f563",
   "metadata": {},
   "source": [
    "### 4. Flatten chunk objects to raw text  \n",
    "We only need the text content for embedding, not the additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0d0af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 883 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "|  \n",
      "  \n",
      "February 2009  \n",
      "  \n",
      "Hacker News was two years old last week. Initially it was supposed to be a\n",
      "side project—an application to sharpen Arc on, and a place for current and\n",
      "future Y Combinator founders to exchange news. It's grown bigger and taken up\n",
      "more time than I expected, but I don't regret that because I've learned so\n",
      "much from working on it.  \n",
      "  \n",
      "**Growth**  \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  \n",
      "When we launched in February 2007, weekday traffic was around 1600 daily\n",
      "uniques. It's since grown to around 22,000. This growth rate is a bit higher\n",
      "than I'd like. I'd like the site to grow, since a site that isn't growing at\n",
      "least slowly is probably dead. But I wouldn't want it to grow as large as Digg\n",
      "or Reddit—mainly because that would dilute the character of the site, but also\n",
      "because I don't want to spend all my time dealing with scaling.  \n",
      "  \n",
      "I already have problems enough with that. Remember, the original motivation\n",
      "for HN was to test a new programming language, and moreover one that's focused\n",
      "on experimenting with language design, not performance. Every time the site\n",
      "gets slow, I fortify myself by recalling McIlroy and Bentley's famous quote\n",
      "\n",
      "> The key to performance is elegance, not battalions of special cases.\n",
      "\n",
      "and look for the bottleneck I can remove with least code. So far I've been\n",
      "able to keep up, in the sense that performance has remained consistently\n",
      "mediocre despite 14x growth. I don't know what I'll do next, but I'll probably\n",
      "think of something.  \n",
      "  \n",
      "This is my attitude to the site generally. Hacker News is an experiment, and\n",
      "an experiment in a very young field. Sites of this type are only a few years\n",
      "old. Internet conversation generally is only a few decades old. So we've\n",
      "probably only discovered a fraction of what we eventually will.  \n",
      "  \n",
      "That's why I'm so optimistic about HN. When a technology is this young, the\n",
      "existing solutions are usually terrible; which means it must be possible to do\n",
      "much better; which means many problems that seem insoluble aren't. Including,\n",
      "I hope, the problem that has afflicted so many previous communities: being\n",
      "ruined by growth.  \n",
      "  \n",
      "**Dilution**  \n",
      "  \n",
      "Users have worried about that since the site was a few months old. So far\n",
      "these alarms have been false, but they may not always be. Dilution is a hard\n",
      "problem. But probably soluble; it doesn't mean much that open conversations\n",
      "have \"always\" been destroyed by growth when \"always\" equals 20 instances.  \n",
      "  \n",
      "But it's important to remember we're trying to solve a new problem, because\n",
      "that means we're going to have to try new things, most of which probably won't\n",
      "work. A couple weeks ago I tried displaying the names of users with the\n",
      "highest average comment scores in orange.  That was a mistake. Suddenly a\n",
      "culture that had been more or less united was divided into haves and have-\n",
      "nots. I didn't realize how united the culture had been till I saw it divided.\n",
      "It was painful to watch.   \n",
      "  \n",
      "So orange usernames won't be back. (Sorry about that.) But there will be other\n",
      "equally broken-seeming ideas in the future, and the ones that turn out to work\n",
      "will probably seem just as broken as those that don't.  \n",
      "  \n",
      "Probably the most important thing I've learned about dilution is that it's\n",
      "measured more in behavior than users. It's bad behavior you want to keep out\n",
      "more than bad people. User behavior turns out to be surprisingly malleable. If\n",
      "people are expected to behave well, they tend to; and vice versa.  \n",
      "  \n",
      "Though of course forbidding bad behavior does tend to keep away bad people,\n",
      "because they feel uncomfortably constrained in a place where they have to\n",
      "behave well. But this way of keeping them out is gentler and probably also\n",
      "more effective than overt barriers.  \n",
      "  \n",
      "It's pretty clear now that the broken windows theory applies to community\n",
      "sites as well. The theory is that minor forms of bad behavior encourage worse\n",
      "ones: that a neighborhood with lots of graffiti and broken windows becomes one\n",
      "where robberies occur. I was living in New York when Giuliani introduced the\n",
      "reforms that made the broken windows theory famous, and the transformation was\n",
      "miraculous. And I was a Reddit user when the opposite happened there, and the\n",
      "transformation was equally dramatic.  \n",
      "  \n",
      "I'm not criticizing Steve and Alexis. What happened to Reddit didn't happen\n",
      "out of neglect. From the start they had a policy of censoring nothing except\n",
      "spam. Plus Reddit had different goals from Hacker News. Reddit was a startup,\n",
      "not a side project; its goal was to grow as fast as possible. Combine rapid\n",
      "growth and zero censorship, and the result is a free for all. But I don't\n",
      "think they'd do much differently if they were doing it again. Measured by\n",
      "traffic, Reddit is much more successful than Hacker News.  \n",
      "  \n",
      "But what happened to Reddit won't inevitably happen to HN. There are several\n",
      "local maxima. \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chunk_texts = [chunk.text for chunks in batch_chunks for chunk in chunks]\n",
    "\n",
    "print(\"Number of chunks:\", len(chunk_texts), \"\\n\" + \"-\"*100)\n",
    "for chunk in chunk_texts[:2]:\n",
    "    print(chunk)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae9862",
   "metadata": {},
   "source": [
    "### 5. Create embeddings with Inference.net  \n",
    "We hit the Inference.net `/v1/embeddings` endpoint (OpenAI-compatible) in mini-batches of 32. You can use any batch size you want, but batch sizes that are too large may case requests to be slow or fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1eecedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.inference.net/v1\",\n",
    "    api_key=os.environ.get(\"INFERENCE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Process embeddings in batches of 32\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(chunk_texts), batch_size):\n",
    "    batch = chunk_texts[i:i + batch_size]\n",
    "    response = client.embeddings.create(\n",
    "        model=\"qwen/qwen3-embedding-4b\",\n",
    "        input=batch\n",
    "    )\n",
    "    batch_embeddings = [data.embedding for data in response.data]\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "embeddings = all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06fe3b",
   "metadata": {},
   "source": [
    "### 6. Persist the vectors in an in-memory Chroma collection  \n",
    "`EphemeralClient` keeps everything in RAM—perfect for demos; switch to a persistent client in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's insert into chroma\n",
    "chroma_client = chromadb.EphemeralClient() # Note that this is in memory and not suitable for production. Use a persistent client, a cloud client, or a completely different vector store in production.\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"paul_graham_all_essays\",\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"dimension\": 2560}\n",
    ")\n",
    "\n",
    "collection.add(\n",
    "    documents=chunk_texts,\n",
    "    embeddings=embeddings,\n",
    "    ids=[str(i) for i in range(len(chunk_texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf4ff3",
   "metadata": {},
   "source": [
    "### 7. Helper: `rag_query()`  \n",
    "Given a natural-language question, we:  \n",
    "1. Embed the query  \n",
    "2. Retrieve the top-K nearest chunks  \n",
    "3. Feed **both** the question and retrieved context into an LLM  \n",
    "4. Return the answer + a DataFrame of retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ebca890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, *, k: int = 3, temperature: float = 0.3):\n",
    "    \"\"\"\n",
    "    Return (answer, DataFrame-of-retrieved-chunks).\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_vec = client.embeddings.create(\n",
    "        model=\"qwen/qwen3-embedding-4b\",\n",
    "        input=question\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Retrieve top-k chunks\n",
    "    res = collection.query(\n",
    "        query_embeddings=[query_vec],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Feed chunks to the LLM\n",
    "    context = \"\\n\\n\".join(res[\"documents\"][0])\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.1-8b-instruct/fp-8\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Question: {question}\\nContext:\\n{context}\\n\\nAnswer:\"}],\n",
    "        temperature=temperature\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": res[\"documents\"][0], \"distance\": res[\"distances\"][0]}\n",
    "    )\n",
    "\n",
    "    return completion, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40ba55",
   "metadata": {},
   "source": [
    "### 8. Example: “What does Paul Graham consider the meaning of work?”  \n",
    "We fire a single RAG query and print both the answer and what text chunks were actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f49de4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Paul Graham, the meaning of work is not just about doing something to earn a living, but about finding something that you are passionate about and enjoy doing. He argues that people who do great work are often those who have found a way to make their work feel like a project of their own, rather than just a chore.\n",
      "\n",
      "Graham identifies three key ingredients for great work: natural ability, practice, and effort. He notes that while natural ability can be an asset, it is not enough on its own, and that practice and effort are essential for achieving great results.\n",
      "\n",
      "Graham also emphasizes the importance of finding work that you love, and that this is not just a matter of doing what you would like to do at any given moment, but about finding something that you can be passionate about and enjoy doing over a longer period of time.\n",
      "\n",
      "He suggests that people should aim to find work that is challenging and meaningful, and that they should be willing to take risks and face challenges in order to achieve their goals. He also notes that finding work that you love can be a difficult and time-consuming process, and that it may involve trying out different things and making mistakes along the way.\n",
      "\n",
      "Ultimately, Graham's view is that the meaning of work is not just about earning a living, but about finding a way to make a positive contribution to the world, and to find a sense of purpose and fulfillment in one's work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n  \\nThe reason some subjects seemed easy wa...</td>\n",
       "      <td>0.457119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nTo do something well you have to like it. ...</td>\n",
       "      <td>0.467175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|  \\n  \\nJune 2021  \\n  \\nA few days ago, on t...</td>\n",
       "      <td>0.529098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  distance\n",
       "0   \\n  \\nThe reason some subjects seemed easy wa...  0.457119\n",
       "1    \\nTo do something well you have to like it. ...  0.467175\n",
       "2  |  \\n  \\nJune 2021  \\n  \\nA few days ago, on t...  0.529098"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, df = rag_query(\"What does Paul Graham consider the meaning of work?\")\n",
    "print(answer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44f563",
   "metadata": {},
   "source": [
    "> **That’s it!** You now have a fully-working, end-to-end Retrieval-Augmented Generation pipeline using:  \n",
    "> • HuggingFace datasets → text source  \n",
    "> • Chonkie → semantic chunking  \n",
    "> • Inference.net → embeddings & LLM completions  \n",
    "> • Chroma → vector storage and similarity search  \n",
    "> Feel free to swap in your own dataset, vector database, or target LLM to customise the workflow for your use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
