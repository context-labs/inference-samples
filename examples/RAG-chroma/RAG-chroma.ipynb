{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05474810",
   "metadata": {},
   "source": [
    "## RAG with ChromaDB, Chonkie, and Paul Graham's Essays\n",
    "\n",
    "Welcome! In this notebook, we’re building a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "\n",
    "Here’s what we’ll use:\n",
    "- ChromaDB for fast vector search\n",
    "- Chonkie for smart, semantic text chunking\n",
    "- OpenAI for embeddings and LLM completions\n",
    "- A dataset of Paul Graham’s essays (because they’re awesome)\n",
    "\n",
    "What you’ll learn:\n",
    "1. How to load and peek at a real-world essay dataset\n",
    "2. How to chunk text in a way that actually makes sense for retrieval\n",
    "3. How to store and search those chunks in a vector DB\n",
    "4. How to use retrieved context to make your LLM answers way better\n",
    "\n",
    "Let’s dive in 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19986a",
   "metadata": {},
   "source": [
    "### 1. Install dependencies and load data\n",
    "We’ll need the OpenAI-compatible SDK to interact with Inference.net, Chroma for vector storage, the `datasets` library for fetching data, and **Chonkie** for smart chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai chromadb datasets chonkie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311775c",
   "metadata": {},
   "source": [
    "### 2. Import Dependencies and Load the Paul Graham essays dataset\n",
    "\n",
    "We pull the full set of essays from 🤗 Hub, convert it to a Pandas DataFrame, and do a quick sanity check on the row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7eb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>|  \\n  \\nFebruary 2009  \\n  \\nHacker News was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>|  \\n  \\nMay 2008  \\n  \\nAdults lie constantly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|  \\n  \\nNovember 2008  \\n  \\nOne of the diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>|  \\n  \\nDecember 2010  \\n  \\nI was thinking r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|  \\n  \\n|  **Want to start a startup?** Get f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  |  \\n  \\nFebruary 2009  \\n  \\nHacker News was ...\n",
       "1  |  \\n  \\nMay 2008  \\n  \\nAdults lie constantly...\n",
       "2  |  \\n  \\nNovember 2008  \\n  \\nOne of the diffe...\n",
       "3  |  \\n  \\nDecember 2010  \\n  \\nI was thinking r...\n",
       "4  |  \\n  \\n|  **Want to start a startup?** Get f..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "from chonkie import SemanticChunker\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"pookie3000/paul_graham_all_essays\")\n",
    "ds = ds[\"train\"].to_pandas()\n",
    "print(len(ds))\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2af77",
   "metadata": {},
   "source": [
    "### 3. Chunk the essays semantically  \n",
    "`SemanticChunker` splits each essay into overlapping, semantically-coherent chunks—useful when you want to do retrieval at paragraph-level rather than whole-essay-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3a8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 chooooooooo           nk  48% • 106/222 docs chunked [00:01<00:01, 80.33doc/s] 🌱/Users/michaelryaboy/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/chonkie/embeddings/model2vec.py:63: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.divide(\n",
      "🦛 choooooooooooooooooooonk 100% • 222/222 docs chunked [00:02<00:00, 93.41doc/s] 🌱 \n"
     ]
    }
   ],
   "source": [
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.47,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=5000,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")\n",
    "\n",
    "\n",
    "batch_chunks = chunker.chunk_batch(ds[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0f563",
   "metadata": {},
   "source": [
    "### 4. Flatten chunk objects to raw text  \n",
    "We only need the text content for embedding, not the additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 883 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "|  \n",
      "  \n",
      "February 2009  \n",
      "  \n",
      "Hacker News was two years old last week. Initially it was supposed to be a\n",
      "side project—an application to sharpen Arc on, and a place for current and\n",
      "future Y Combinator founders to exchange news. It's grown bigger and taken up\n",
      "more time than I expected, but I don't regret \n",
      "----------------------------------------------------------------------------------------------------\n",
      "  \n",
      "When we launched in February 2007, weekday traffic was around 1600 daily\n",
      "uniques. It's since grown to around 22,000. This growth rate is a bit higher\n",
      "than I'd like. I'd like the site to grow, since a site that isn't growing at\n",
      "least slowly is probably dead. But I wouldn't want it to grow as large\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chunk_texts = [chunk.text for chunks in batch_chunks for chunk in chunks]\n",
    "\n",
    "print(\"Number of chunks:\", len(chunk_texts), \"\\n\" + \"-\"*100)\n",
    "for chunk in chunk_texts[:2]:\n",
    "    print(chunk[:300])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae9862",
   "metadata": {},
   "source": [
    "### 5. Create embeddings with Inference.net  \n",
    "We hit the Inference.net `/v1/embeddings` endpoint (OpenAI-compatible) in mini-batches of 32. You can use any batch size you want, but batch sizes that are too large may case requests to be slow or fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eecedb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'This model is out of capacity, please try again later', 'type': 'InferenceRateLimitError'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chunk_texts), batch_size):\n\u001b[1;32m     13\u001b[0m     batch \u001b[38;5;241m=\u001b[39m chunk_texts[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen/qwen3-embedding-4b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata]\n\u001b[1;32m     19\u001b[0m     all_embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n",
      "File \u001b[0;32m~/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/openai/resources/embeddings.py:129\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    123\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    124\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/openai/_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/recent-projects/inference-webhook/venv/lib/python3.9/site-packages/openai/_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1036\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'This model is out of capacity, please try again later', 'type': 'InferenceRateLimitError'}}"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.inference.net/v1\",\n",
    "    api_key=os.environ.get(\"INFERENCE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Process embeddings in batches of 32\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(0, len(chunk_texts), batch_size):\n",
    "    batch = chunk_texts[i:i + batch_size]\n",
    "    response = client.embeddings.create(\n",
    "        model=\"qwen/qwen3-embedding-4b\",\n",
    "        input=batch\n",
    "    )\n",
    "    batch_embeddings = [data.embedding for data in response.data]\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    time.sleep(1)\n",
    "\n",
    "embeddings = all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06fe3b",
   "metadata": {},
   "source": [
    "### 6. Persist the vectors in an in-memory Chroma collection  \n",
    "`EphemeralClient` keeps everything in RAM—perfect for demos; switch to a persistent client in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's insert into chroma\n",
    "chroma_client = chromadb.EphemeralClient() # Note that this is in memory and not suitable for production. Use a persistent client, a cloud client, or a completely different vector store in production.\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"paul_graham_all_essays\",\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"dimension\": 2560}\n",
    ")\n",
    "\n",
    "collection.add(\n",
    "    documents=chunk_texts,\n",
    "    embeddings=embeddings,\n",
    "    ids=[str(i) for i in range(len(chunk_texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf4ff3",
   "metadata": {},
   "source": [
    "### 7. Helper: `rag_query()`  \n",
    "Given a natural-language question, we:  \n",
    "1. Embed the query  \n",
    "2. Retrieve the top-K nearest chunks  \n",
    "3. Feed **both** the question and retrieved context into an LLM  \n",
    "4. Return the answer + a DataFrame of retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, *, k: int = 3, temperature: float = 0.3):\n",
    "    \"\"\"\n",
    "    Return (answer, DataFrame-of-retrieved-chunks).\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_vec = client.embeddings.create(\n",
    "        model=\"qwen/qwen3-embedding-4b\",\n",
    "        input=question\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Retrieve top-k chunks\n",
    "    res = collection.query(\n",
    "        query_embeddings=[query_vec],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Feed chunks to the LLM\n",
    "    context = \"\\n\\n\".join(res[\"documents\"][0])\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.1-8b-instruct/fp-8\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Question: {question}\\nContext:\\n{context}\\n\\nAnswer:\"}],\n",
    "        temperature=temperature\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": res[\"documents\"][0], \"distance\": res[\"distances\"][0]}\n",
    "    )\n",
    "\n",
    "    return completion, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40ba55",
   "metadata": {},
   "source": [
    "### 8. Example: “What does Paul Graham consider the meaning of work?”  \n",
    "We fire a single RAG query and print both the answer and what text chunks were actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49de4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Paul Graham, the meaning of work is not just about doing something to earn a living, but about finding something that you are passionate about and enjoy doing. He argues that people who do great work are often those who have found a way to make their work feel like a project of their own, rather than just a chore.\n",
      "\n",
      "Graham identifies three key ingredients for great work: natural ability, practice, and effort. He notes that while natural ability can be an asset, it is not enough on its own, and that practice and effort are essential for achieving great results.\n",
      "\n",
      "Graham also emphasizes the importance of finding work that you love, and that this is not just a matter of doing what you would like to do at any given moment, but about finding something that you can be passionate about and enjoy doing over a longer period of time.\n",
      "\n",
      "He suggests that people should aim to find work that is challenging and meaningful, and that they should be willing to take risks and face challenges in order to achieve their goals. He also notes that finding work that you love can be a difficult and time-consuming process, and that it may involve trying out different things and making mistakes along the way.\n",
      "\n",
      "Ultimately, Graham's view is that the meaning of work is not just about earning a living, but about finding a way to make a positive contribution to the world, and to find a sense of purpose and fulfillment in one's work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n  \\nThe reason some subjects seemed easy wa...</td>\n",
       "      <td>0.457119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nTo do something well you have to like it. ...</td>\n",
       "      <td>0.467175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|  \\n  \\nJune 2021  \\n  \\nA few days ago, on t...</td>\n",
       "      <td>0.529098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  distance\n",
       "0   \\n  \\nThe reason some subjects seemed easy wa...  0.457119\n",
       "1    \\nTo do something well you have to like it. ...  0.467175\n",
       "2  |  \\n  \\nJune 2021  \\n  \\nA few days ago, on t...  0.529098"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, df = rag_query(\"What does Paul Graham consider the meaning of work?\")\n",
    "print(answer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44f563",
   "metadata": {},
   "source": [
    "> **That’s it!** You now have a fully-working, end-to-end Retrieval-Augmented Generation pipeline using:  \n",
    "> • HuggingFace datasets → text source  \n",
    "> • Chonkie → semantic chunking  \n",
    "> • Inference.net → embeddings & LLM completions  \n",
    "> • Chroma → vector storage and similarity search  \n",
    "> Feel free to swap in your own dataset, vector database, or target LLM to customise the workflow for your use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
