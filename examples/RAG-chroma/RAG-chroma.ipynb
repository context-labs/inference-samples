{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05474810",
   "metadata": {},
   "source": [
    "## RAG with ChromaDB, Chonkie, and Paul Graham's Essays\n",
    "\n",
    "Welcome! In this notebook, we’re building a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "\n",
    "Here’s what we’ll use:\n",
    "- ChromaDB for fast vector search\n",
    "- Chonkie for smart, semantic text chunking\n",
    "- OpenAI for embeddings and LLM completions\n",
    "- A dataset of Paul Graham’s essays (because they’re awesome)\n",
    "\n",
    "What you’ll learn:\n",
    "1. How to load and peek at a real-world essay dataset\n",
    "2. How to chunk text in a way that actually makes sense for retrieval\n",
    "3. How to store and search those chunks in a vector DB\n",
    "4. How to use retrieved context to make your LLM answers way better\n",
    "\n",
    "Let’s dive in 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19986a",
   "metadata": {},
   "source": [
    "### 1. Install dependencies and load data\n",
    "We’ll need the OpenAI-compatible SDK to interact with Inference.net, Chroma for vector storage, the `datasets` library for fetching data, and **Chonkie** for smart chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai chromadb datasets chonkie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311775c",
   "metadata": {},
   "source": [
    "### 2. Import Dependencies and Load the Paul Graham essays dataset\n",
    "\n",
    "We pull the full set of essays from 🤗 Hub, convert it to a Pandas DataFrame, and do a quick sanity check on the row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7eb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>|  \\n  \\nFebruary 2009  \\n  \\nHacker News was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>|  \\n  \\nMay 2008  \\n  \\nAdults lie constantly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|  \\n  \\nNovember 2008  \\n  \\nOne of the diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>|  \\n  \\nDecember 2010  \\n  \\nI was thinking r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|  \\n  \\n|  **Want to start a startup?** Get f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  |  \\n  \\nFebruary 2009  \\n  \\nHacker News was ...\n",
       "1  |  \\n  \\nMay 2008  \\n  \\nAdults lie constantly...\n",
       "2  |  \\n  \\nNovember 2008  \\n  \\nOne of the diffe...\n",
       "3  |  \\n  \\nDecember 2010  \\n  \\nI was thinking r...\n",
       "4  |  \\n  \\n|  **Want to start a startup?** Get f..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "from chonkie import SemanticChunker\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"pookie3000/paul_graham_all_essays\")\n",
    "ds = ds[\"train\"].to_pandas()\n",
    "print(len(ds))\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2af77",
   "metadata": {},
   "source": [
    "### 3. Chunk the essays semantically  \n",
    "`SemanticChunker` splits each essay into overlapping, semantically-coherent chunks—useful when you want to do retrieval at paragraph-level rather than whole-essay-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.47,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=5000,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")\n",
    "\n",
    "\n",
    "batch_chunks = chunker.chunk_batch(ds[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0f563",
   "metadata": {},
   "source": [
    "### 4. Flatten chunk objects to raw text  \n",
    "We only need the text content for embedding, not the additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d0af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 883 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "|  \n",
      "  \n",
      "February 2009  \n",
      "  \n",
      "Hacker News was two years old last week. Initially it was supposed to be a\n",
      "side project—an application to sharpen Arc on, and a place for current and\n",
      "future Y Combinator founders to exchange news. It's grown bigger and taken up\n",
      "more time than I expected, but I don't regret \n",
      "----------------------------------------------------------------------------------------------------\n",
      "  \n",
      "When we launched in February 2007, weekday traffic was around 1600 daily\n",
      "uniques. It's since grown to around 22,000. This growth rate is a bit higher\n",
      "than I'd like. I'd like the site to grow, since a site that isn't growing at\n",
      "least slowly is probably dead. But I wouldn't want it to grow as large\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chunk_texts = [chunk.text for chunks in batch_chunks for chunk in chunks]\n",
    "\n",
    "print(\"Number of chunks:\", len(chunk_texts), \"\\n\" + \"-\"*100)\n",
    "for chunk in chunk_texts[:2]:\n",
    "    print(chunk[:300])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae9862",
   "metadata": {},
   "source": [
    "### 5. Create embeddings with Inference.net  \n",
    "We hit the Inference.net `/v1/embeddings` endpoint (OpenAI-compatible) in mini-batches of 32. You can use any batch size you want, but batch sizes that are too large may case requests to be slow or fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.inference.net/v1\",\n",
    "    api_key=os.environ.get(\"INFERENCE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Process embeddings in batches of 32\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Embedding batches\"):\n",
    "    batch = chunk_texts[i:i + batch_size]\n",
    "    response = client.embeddings.create(\n",
    "        model=\"qwen/qwen3-embedding-4b\",\n",
    "        input=batch\n",
    "    )\n",
    "    batch_embeddings = [data.embedding for data in response.data]\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    time.sleep(1)\n",
    "\n",
    "embeddings = all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06fe3b",
   "metadata": {},
   "source": [
    "### 6. Persist the vectors in an in-memory Chroma collection  \n",
    "`EphemeralClient` keeps everything in RAM—perfect for demos; switch to a persistent client in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's insert into chroma\n",
    "chroma_client = chromadb.EphemeralClient() # Note that this is in memory and not suitable for production. Use a persistent client, a cloud client, or a completely different vector store in production.\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"paul_graham_all_essays\",\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"dimension\": 2560}\n",
    ")\n",
    "\n",
    "collection.add(\n",
    "    documents=chunk_texts,\n",
    "    embeddings=embeddings,\n",
    "    ids=[str(i) for i in range(len(chunk_texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf4ff3",
   "metadata": {},
   "source": [
    "### 7. Helper: `rag_query()`  \n",
    "Given a natural-language question, we:  \n",
    "1. Embed the query  \n",
    "2. Retrieve the top-K nearest chunks  \n",
    "3. Feed **both** the question and retrieved context into an LLM  \n",
    "4. Return the answer + a DataFrame of retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebca890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, *, k: int = 3, temperature: float = 0.3):\n",
    "    \"\"\"\n",
    "    Return (answer, DataFrame-of-retrieved-chunks).\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_vec = client.embeddings.create(\n",
    "        model=\"qwen/qwen3-embedding-4b\",\n",
    "        input=question\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Retrieve top-k chunks\n",
    "    res = collection.query(\n",
    "        query_embeddings=[query_vec],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Feed chunks to the LLM\n",
    "    context = \"\\n\\n\".join(res[\"documents\"][0])\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.1-8b-instruct/fp-8\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Question: {question}\\nContext:\\n{context}\\n\\nAnswer:\"}],\n",
    "        temperature=temperature\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": res[\"documents\"][0], \"distance\": res[\"distances\"][0]}\n",
    "    )\n",
    "\n",
    "    return completion, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40ba55",
   "metadata": {},
   "source": [
    "### 8. Example: “What does Paul Graham consider the meaning of work?”  \n",
    "We fire a single RAG query and print both the answer and what text chunks were actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49de4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Paul Graham, the meaning of work is not just about doing something to make a living, but about finding something that you are passionate about and enjoy doing. He argues that the idea of \"do what you love\" is not just a cliché, but a fundamental principle for achieving success and happiness.\n",
      "\n",
      "Graham emphasizes that the key to finding meaningful work is to understand the shape of real work, which involves learning to distinguish between real work and fake work. He notes that school often distorts the nature of work, making it seem boring and pointless. However, when people learn what real work is, they can develop a passion for it.\n",
      "\n",
      "Graham also stresses the importance of discipline and hard work in achieving success. He argues that working hard is not just about putting in long hours, but about being focused and motivated to achieve a specific goal. He notes that the key to working hard is to find a problem that you are passionate about and to be willing to put in the effort to solve it.\n",
      "\n",
      "Furthermore, Graham emphasizes the importance of finding work that aligns with your values and interests. He argues that people should not just focus on making a living, but on finding work that gives them a sense of purpose and fulfillment.\n",
      "\n",
      "In terms of the relationship between work and happiness, Graham notes that people often underestimate the importance of finding work that they enjoy. He argues that happiness is not just about doing what you love, but about doing what you are good at and what gives you a sense of purpose.\n",
      "\n",
      "Overall, Graham's view of the meaning of work is that it is about finding something that you are passionate about, enjoy doing, and are good at. He emphasizes the importance of discipline, hard work, and finding work that aligns with your values and interests in achieving success and happiness.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n  \\nThe reason some subjects seemed easy wa...</td>\n",
       "      <td>0.457119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nTo do something well you have to like it. ...</td>\n",
       "      <td>0.466736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|  \\n  \\nJune 2021  \\n  \\nA few days ago, on t...</td>\n",
       "      <td>0.529098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  distance\n",
       "0   \\n  \\nThe reason some subjects seemed easy wa...  0.457119\n",
       "1    \\nTo do something well you have to like it. ...  0.466736\n",
       "2  |  \\n  \\nJune 2021  \\n  \\nA few days ago, on t...  0.529098"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, df = rag_query(\"What does Paul Graham consider the meaning of work?\")\n",
    "print(answer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44f563",
   "metadata": {},
   "source": [
    "> **That’s it!** You now have a fully-working, end-to-end Retrieval-Augmented Generation pipeline using:  \n",
    "> • HuggingFace datasets → text source  \n",
    "> • Chonkie → semantic chunking  \n",
    "> • Inference.net → embeddings & LLM completions  \n",
    "> • Chroma → vector storage and similarity search  \n",
    "> Feel free to swap in your own dataset, vector database, or target LLM to customise the workflow for your use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
