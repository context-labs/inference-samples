{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962b568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/michaelryaboy/recent-projects/inference-webhook/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88c1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: hBrvLzemNvx8hzb0mXQ1O\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=0, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=18, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=766, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=1287, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=1591, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=2072, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=2619, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=3023, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=3349, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=3981, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=4396, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=4762, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=5269, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=5867, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=6607, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=7193, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=7964, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=8624, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=9274, total=10000)\n",
      "status: in_progress BatchRequestCounts(completed=0, failed=9916, total=10000)\n",
      "status: completed BatchRequestCounts(completed=0, failed=10000, total=10000)\n",
      "\n",
      "Sample output:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os, json, io, time, random\n",
    "from openai import OpenAI\n",
    "\n",
    "N = 10_000\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.inference.net/v1\",\n",
    "    api_key=os.getenv(\"INFERENCE_API_KEY\"),\n",
    ")\n",
    "\n",
    "buf = io.BytesIO()\n",
    "for i in range(N):\n",
    "    buf.write(json.dumps({\n",
    "        \"custom_id\": f\"req-{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"meta-llama/llama-3.2-1b-instruct/fp-8\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\":\n",
    "                 f\"What is {random.choice(['2+2', 'the capital of France', 'the opposite of hot'])}?\"},\n",
    "            ],\n",
    "            \"max_tokens\": 5,\n",
    "        },\n",
    "    }).encode() + b'\\n')\n",
    "buf.seek(0)\n",
    "buf.name = \"batch.jsonl\"          # <<‑ add a filename so the API sees “.jsonl”\n",
    "\n",
    "file_id = client.files.create(file=buf, purpose=\"batch\").id\n",
    "batch   = client.batches.create(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while batch.status not in {\"completed\", \"failed\", \"expired\", \"cancelled\"}:\n",
    "    time.sleep(10)\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"status:\", batch.status, batch.request_counts)\n",
    "\n",
    "if batch.status == \"completed\":\n",
    "    out = client.files.content(batch.output_file_id).text.splitlines()\n",
    "    print(\"\\nSample output:\\n\", \"\\n\".join(out[:5]))\n",
    "else:\n",
    "    print(\"Batch ended with status:\", batch.status)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
